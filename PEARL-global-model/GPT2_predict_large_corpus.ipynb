{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKYC1d_h-1T7"
      },
      "source": [
        "## CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvgme1E3IJfB",
        "outputId": "e2ef5ebe-bc13-4bdc-ae3d-374776a433d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-dev libnvinfer-plugin7 libnvinfer7\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 143 MB of archives.\n",
            "After this operation, 520 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer7 7.0.0-1+cuda10.0 [69.6 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-dev 7.0.0-1+cuda10.0 [71.6 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-plugin7 7.0.0-1+cuda10.0 [2,109 kB]\n",
            "Fetched 143 MB in 2s (64.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer7.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvinfer7_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer7 (7.0.0-1+cuda10.0) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../libnvinfer-dev_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (7.0.0-1+cuda10.0) ...\n",
            "Selecting previously unselected package libnvinfer-plugin7.\n",
            "Preparing to unpack .../libnvinfer-plugin7_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin7 (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer7 (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer-dev (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer-plugin7 (7.0.0-1+cuda10.0) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y --no-install-recommends libnvinfer7=7.0.0-1+cuda10.0 \\\n",
        "    libnvinfer-dev=7.0.0-1+cuda10.0 \\\n",
        "    libnvinfer-plugin7=7.0.0-1+cuda10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Installing necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "5ab477c7-277f-4a39-e499-71fdd484c5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 32 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.13.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.42.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=3bbdf0a9f8f296f86687f1a0a91d0d935ec90b7054a033d3a56ef1e1ee174bc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow-gpu\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 tensorflow-gpu-1.15.2\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "#!pip install tensorflow==1.15.2\n",
        "!pip install tensorflow-gpu==1.15.2\n",
        "!pip install -q gpt-2-simple==0.7.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQFo2PINvblo",
        "outputId": "5d9e76d9-4081-4faa-da86-f905f4039554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## Verifying GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmTooTW3osf",
        "outputId": "0c3afcd4-731d-4df5-bdbc-000f4cd2f3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 23 20:43:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgvl-BJ7I4H",
        "outputId": "b5224449-5c24-407b-962c-0520be247c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11559251388955632759\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 1260313731705644774\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 922248498910722416\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15964005991\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7960961174839188233\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxKEhGLhQ2H0",
        "outputId": "01c1e278-de93-4032-8d34-6a886fe4d9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='9000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generating candidates using GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "outputs": [],
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlD93fzPsmo_"
      },
      "outputs": [],
      "source": [
        "from gpt.gpt_manager import GPTManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVeJYgBAXT3h"
      },
      "outputs": [],
      "source": [
        "gpt_manager = GPTManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cb7co1DZthy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414b62cf-e52a-4b76-da2e-c27f1dfa5677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "244 end tokens.\n",
            "Loading model checkpoint/run1/model-150000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-150000\n",
            "WARNING:tensorflow:From /content/gpt/sample.py:33: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ],
      "source": [
        "gpt_manager.java_model = gpt_manager.load_model('checkpoint/', 'run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "outputs": [],
      "source": [
        "main_path = \"drive/MyDrive/shared/GPT/\"\n",
        "repo_dir = \"four_hundred/\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName):\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D8DCdMR7o2f"
      },
      "outputs": [],
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWsJFtYc-Ml_"
      },
      "outputs": [],
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "outputs": [],
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6HyRqvGa6fY"
      },
      "outputs": [],
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaWz9QQsAXoV"
      },
      "outputs": [],
      "source": [
        "def canAcceptGeneratedLexes(test):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    for group in test['next_lex']:\n",
        "        for candidate in group:\n",
        "            candidate = preprocess(candidate)\n",
        "            if matchesArg(expectedLex, candidate):\n",
        "                return True\n",
        "\n",
        "            alternateLex = None\n",
        "            if 'methodAccessLex' in test:\n",
        "                alternateLex = test['methodAccessLex']\n",
        "            if 'objectCreationLex' in test:\n",
        "                alternateLex = test['objectCreationLex']\n",
        "            if alternateLex is not None and matchesArg(alternateLex, candidate):\n",
        "                return True\n",
        "\n",
        "            if 'staticMemberAccessLex' in test:\n",
        "                if matchesArg(test['staticMemberAccessLex'], candidate):\n",
        "                    return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUdqQfcBbA0A"
      },
      "outputs": [],
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqn5PBv8c19j"
      },
      "outputs": [],
      "source": [
        "def validateTest(test):\n",
        "    if not test['ignored']:\n",
        "        adequateGeneratedLex = False\n",
        "        if canAcceptGeneratedLexes(test):\n",
        "            adequateGeneratedLex = True;\n",
        "        if adequateGeneratedLex:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzjVqmp-3Eg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "PREDICT_PERCENT = 100\n",
        "\n",
        "if PREDICT_PERCENT == 100:\n",
        "    os.makedirs('predictions/' + repo_dir + 'gpt2/', exist_ok=True)\n",
        "else:\n",
        "    os.makedirs(f'predictions_{PREDICT_PERCENT}/' + repo_dir + 'gpt2/', exist_ok=True)\n",
        "\n",
        "for testFile in os.listdir(tests_path):\n",
        "    projectName = testFile[:testFile.find(\"_ArgRecTests\")]\n",
        "    if PREDICT_PERCENT == 100:\n",
        "        predictionFilePath = \"predictions/\" + repo_dir + \"gpt2/\" + f\"{projectName}_ArgRecs.txt\"\n",
        "    else:\n",
        "        predictionFilePath = f\"predictions_{PREDICT_PERCENT}/\" + repo_dir + \"gpt2/\" + f\"{projectName}_ArgRecs.txt\"\n",
        "    if os.path.isfile(main_path + predictionFilePath):\n",
        "        continue\n",
        "    if os.path.isfile(main_path + \"predictions/\" + repo_dir + \"gpt2/\" + f\"{projectName}_ArgRecs.txt\"):\n",
        "        continue\n",
        "    print(projectName)\n",
        "    prediction_details = []\n",
        "    tests = allTestsToSingleArgRecTest(readTests(projectName))\n",
        "    tests = tests[: len(tests) * PREDICT_PERCENT // 100]\n",
        "    cnt = -1\n",
        "    for i in tqdm(range(len(tests))):\n",
        "        test = tests[i]\n",
        "        if test['numArg'] == 0:\n",
        "            continue\n",
        "\n",
        "        response = gpt_manager.generate_param(test)\n",
        "        gptResults = response['result']\n",
        "\n",
        "        prediction_detail = {}\n",
        "        prediction_detail['test_id'] = test['id']\n",
        "        prediction_detail['predictions'] = gptResults\n",
        "        prediction_detail['answer'] = test['expected_lex']\n",
        "        prediction_detail['arg_type'] = test['argRecTestList'][0]['argType']\n",
        "        prediction_detail['runtime'] = response['runtime']\n",
        "        prediction_detail['ignored'] = test['ignored']\n",
        "        prediction_detail['sufficient_candidates'] = validateTest(test)\n",
        "        prediction_details.append(prediction_detail)\n",
        "\n",
        "    with open(predictionFilePath, \"w\") as f:\n",
        "        for prediction_detail in prediction_details:\n",
        "            f.write(json.dumps(prediction_detail) + '\\n')\n",
        "\n",
        "    shutil.copyfile(predictionFilePath, main_path + predictionFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "outputs": [],
      "source": [
        "!kill -9 -1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xbulk8mEHdzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "GPT2_predict_large_corpus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}