{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_evaluate_large_corpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe8d99641f8b48f49be4122ca8dc3c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eef09e83e7c247d79cabe9a13338b8f7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c7b613903d7407dbf1524bd014f821a",
              "IPY_MODEL_d45ded54376c4248a51d1cb66275c59d",
              "IPY_MODEL_f661c7c006694ea88767c6a1c2c95350"
            ]
          }
        },
        "eef09e83e7c247d79cabe9a13338b8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c7b613903d7407dbf1524bd014f821a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_216828f3103d4a1e8667091e7a820c8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eddd8ee8292a49ee8f3c78266e7f94c8"
          }
        },
        "d45ded54376c4248a51d1cb66275c59d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7b9e8013b4494fc2abec5d9085a9db82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 387,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 387,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30818dc1571c4cf6a1cd91099d89c7bf"
          }
        },
        "f661c7c006694ea88767c6a1c2c95350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d8fe80cab1243e1a0a4dcbcfabb8d14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 387/387 [15:09&lt;00:00,  6.88s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_429c09077d3a4e1680510ff1a7d8512b"
          }
        },
        "216828f3103d4a1e8667091e7a820c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eddd8ee8292a49ee8f3c78266e7f94c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b9e8013b4494fc2abec5d9085a9db82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30818dc1571c4cf6a1cd91099d89c7bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d8fe80cab1243e1a0a4dcbcfabb8d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "429c09077d3a4e1680510ff1a7d8512b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq4iC6vUAHc",
        "outputId": "bb43b344-787e-42da-b0b1-b0a37a538dda"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "source": [
        "main_path = \"drive/MyDrive/shared/GPT/\"\n",
        "repo_dir = \"four_hundred/\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir\n",
        "model = \"gpt2\"\n",
        "pred_path = main_path + \"predictions/\" + repo_dir + \"gpt2/\"\n",
        "result_path = main_path + \"results/\" + repo_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREDICT_PERCENT = 100"
      ],
      "metadata": {
        "id": "nzJ2qF8xQK03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName):\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ],
      "metadata": {
        "id": "jKjZVhFuX-DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['line'] = this['line']\n",
        "    test['col'] = this['col']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    return test"
      ],
      "metadata": {
        "id": "syDJAophYIkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ],
      "metadata": {
        "id": "PpFtWc-GYL0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "source": [
        "import os\n",
        "\n",
        "def readPredictions(projectName):\n",
        "    predictions = []\n",
        "    if os.path.isfile(f\"{pred_path}{projectName}_ArgRecs.txt\"):\n",
        "        filePath = f\"{pred_path}{projectName}_ArgRecs.txt\"\n",
        "    else:\n",
        "        filePath = main_path + f\"predictions_{PREDICT_PERCENT}/{repo_dir}gpt2/{projectName}_ArgRecs.txt\"\n",
        "\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readAppendices(projectName):\n",
        "    filePath = f\"{main_path}appendices/{repo_dir}{projectName}_ArgRecTestAppendices.txt\"\n",
        "    if not os.path.isfile(filePath):\n",
        "        return None\n",
        "    oneArgTests = []\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ],
      "metadata": {
        "id": "nkfr58CIJ1y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN-nKkwg6Va8"
      },
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and result == \"<LAMBDA>\":\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YpySsDlz_m"
      },
      "source": [
        "def canAcceptResult(test, result):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    result = preprocess(result)\n",
        "    if '{' in result:\n",
        "        result = result[:result.index('{')].rstrip()\n",
        "    if result.find('(') > 0:\n",
        "        result = preprocessor.normalize_method_invocation(result)\n",
        "    \n",
        "    if matchesArg(expectedLex, result):\n",
        "        return True\n",
        "\n",
        "    alternateLex = None\n",
        "    if 'methodAccessLex' in test:\n",
        "        alternateLex = test['methodAccessLex']\n",
        "    if 'objectCreationLex' in test:\n",
        "        alternateLex = test['objectCreationLex']\n",
        "    if alternateLex is not None and matchesArg(alternateLex, result):\n",
        "        return True\n",
        "\n",
        "    if 'staticMemberAccessLex' in test:\n",
        "        if matchesArg(test['staticMemberAccessLex'], result):\n",
        "            return True\n",
        "            \n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUqA-mJ36SFm"
      },
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "os.makedirs('results/' + repo_dir, exist_ok=True)\n",
        "os.makedirs('logs/' + repo_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "L82NnK3naslK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b96CurBBe5ET"
      },
      "source": [
        "dataFrame = defaultdict(list)\n",
        "\n",
        "def updateTopKResult(test, results, k, adequateGeneratedCandidate, doPrintIncorrectPrediction, projectName):\n",
        "    isOverallCorrectTopK = False\n",
        "    for i in range(min(k, len(results))):\n",
        "        if canAcceptResult(test, results[i]):\n",
        "            isOverallCorrectTopK = True\n",
        "            break\n",
        "\n",
        "    argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else 'null'\n",
        "    if isOverallCorrectTopK:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(1)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(1)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(1)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(1)\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(1)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(1)\n",
        "    else:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(0)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(0)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(0)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(0)\n",
        "            if doPrintIncorrectPrediction:\n",
        "                outputFileName = f\"{projectName}_incorrect_ArgRecTests_top_{k}.txt\"\n",
        "                with open(f'logs/{repo_dir}{outputFileName}', \"a\") as f:\n",
        "                    f.write(json.dumps(test['argRecTestList'][0]) + '\\n')\n",
        "                    f.write('Predictions: ' + str(results) + '\\n')\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(0)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fe8d99641f8b48f49be4122ca8dc3c03",
            "eef09e83e7c247d79cabe9a13338b8f7",
            "4c7b613903d7407dbf1524bd014f821a",
            "d45ded54376c4248a51d1cb66275c59d",
            "f661c7c006694ea88767c6a1c2c95350",
            "216828f3103d4a1e8667091e7a820c8f",
            "eddd8ee8292a49ee8f3c78266e7f94c8",
            "7b9e8013b4494fc2abec5d9085a9db82",
            "30818dc1571c4cf6a1cd91099d89c7bf",
            "7d8fe80cab1243e1a0a4dcbcfabb8d14",
            "429c09077d3a4e1680510ff1a7d8512b"
          ]
        },
        "id": "DCzjVqmp-3Eg",
        "outputId": "e4cf9f3e-86f1-4a6d-a351-5f8dba401419"
      },
      "source": [
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "COMPOUND_CONSIDERED = False\n",
        "TEST_INTRA_INTER_MCALL = None\n",
        "if TEST_INTRA_INTER_MCALL is not None:\n",
        "    if TEST_INTRA_INTER_MCALL:\n",
        "        model += \"__intra_mcall\"\n",
        "    else:\n",
        "        model += \"__inter_mcall\"\n",
        "TEST_LOCAL_ARG = None\n",
        "if TEST_LOCAL_ARG is not None:\n",
        "    if TEST_LOCAL_ARG:\n",
        "        model += \"__local_var\"\n",
        "    else:\n",
        "        model += \"__instance_var_class_var\"\n",
        "\n",
        "rank_list = []\n",
        "\n",
        "# for predId in tqdm(range(len(os.listdir(pred_path)))):\n",
        "#     predFile = os.listdir(pred_path)[predId]\n",
        "#     projectName = predFile[:predFile.find(\"_ArgRecs\")]\n",
        "for projectId in tqdm(range(len(os.listdir(tests_path)))):\n",
        "    testFile = os.listdir(tests_path)[projectId]\n",
        "    projectName = testFile[:testFile.find(\"_ArgRecTests\")]\n",
        "    tests = allTestsToSingleArgRecTest(readTests(projectName))\n",
        "    predictions = readPredictions(projectName)\n",
        "\n",
        "    if TEST_INTRA_INTER_MCALL is not None:\n",
        "        appendices = readAppendices(projectName)\n",
        "        if appendices is None:\n",
        "            print(f\"Appendices for {projectName} not found!\")\n",
        "            continue\n",
        "        mcall_type_set = set()\n",
        "        for appendice in appendices:\n",
        "            if TEST_INTRA_INTER_MCALL:\n",
        "                if appendice['methodInvocOrigin'] == \"src\":\n",
        "                    mcall_type_set.add(f\"{appendice['filePath']}-{appendice['line']}-{appendice['col']}\")\n",
        "            else:\n",
        "                if appendice['methodInvocOrigin'] != \"src\":\n",
        "                    mcall_type_set.add(f\"{appendice['filePath']}-{appendice['line']}-{appendice['col']}\")\n",
        "\n",
        "    tests = tests[: len(tests) * PREDICT_PERCENT // 100]\n",
        "    for i in range(min(len(tests), len(predictions))):\n",
        "        test = tests[i]\n",
        "        prediction_detail = predictions[i]\n",
        "        dataFrame['Tested'].append(1)\n",
        "\n",
        "        if test['numArg'] == 0:\n",
        "            continue\n",
        "        if TEST_INTRA_INTER_MCALL is not None:\n",
        "            if f\"{test['filePath']}-{test['line']}-{test['col']}\" not in mcall_type_set:\n",
        "              continue\n",
        "        if TEST_LOCAL_ARG is not None:\n",
        "            if not predictions[i]['sufficient_candidates']:\n",
        "                continue\n",
        "            if test['argRecTestList'][0]['argType'] != 'NAME':\n",
        "                continue\n",
        "            is_local_arg = False\n",
        "            for j in range(len(test['next_lex'][0])):\n",
        "                for k in range(len(test['next_lex'][0][j])):\n",
        "                    candidate = test['next_lex'][0][j][k]\n",
        "                    if candidate == test['expected_lex']:\n",
        "                        candidate_locality = test['argRecTestList'][0]['candidates_locality'][j][k]\n",
        "                        if candidate_locality >= 4:\n",
        "                            is_local_arg = True\n",
        "                            break\n",
        "                if is_local_arg:\n",
        "                    break\n",
        "            if TEST_LOCAL_ARG != is_local_arg:\n",
        "                continue\n",
        "\n",
        "        dataFrame['Predicted'].append(1)\n",
        "        if not test['ignored']:\n",
        "            dataFrame['Predicted supported'].append(1)\n",
        "        \n",
        "        gptResults = prediction_detail['predictions']\n",
        "        if not COMPOUND_CONSIDERED:\n",
        "            if '<COMPOUND>' in gptResults:\n",
        "                gptResults.remove('<COMPOUND>')\n",
        "\n",
        "        rank = -1\n",
        "        for k in range(min(10, len(gptResults))):\n",
        "            if canAcceptResult(test, gptResults[k]):\n",
        "                rank = k\n",
        "        rank_list.append(rank)\n",
        "\n",
        "        for k in tops:\n",
        "            updateTopKResult(test, gptResults, k, prediction_detail['sufficient_candidates'], False, projectName)\n",
        "\n",
        "        dataFrame[\"GPT's runtime\"].append(prediction_detail['runtime'])\n",
        "        argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else None\n",
        "        if argType is not None:\n",
        "            dataFrame[\"ArgType\"].append(expressionTypeDict[argType])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe8d99641f8b48f49be4122ca8dc3c03",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MRR = 0\n",
        "for rank in rank_list:\n",
        "    if rank < 0:\n",
        "        MRR += 0\n",
        "    else:\n",
        "        MRR += 1/(rank + 1)\n",
        "MRR /= len(rank_list)\n",
        "print(MRR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIIctu-A4RSh",
        "outputId": "12499e49-f93a-42db-993b-f6e9b3c1e887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4369313030260945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtmIUfQaNrr-"
      },
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('logs/dataframe.pkl', 'wb') as f:\n",
        "#     pickle.dump(dataFrame, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXCB5cWdvYKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0927a7df-43c7-466d-b409-a883bceecd16"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def printTestResult():\n",
        "    with open(f'results/{repo_dir}arg_rec_{model}_log.txt', 'w') as f:\n",
        "        f.write(f\"Ran {len(dataFrame['Tested'])} tests successfully.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted'])} tests.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted supported'])} tests that were supported.\\n\")\n",
        "        f.write(f\"Skipped {len(dataFrame['Tested']) - len(dataFrame['Predicted'])} tests. They were not taken into account during evaluation.\\n\")\n",
        "        gptRuntime = np.mean(dataFrame[\"GPT's runtime\"])\n",
        "        f.write(f\"GPT's runtime: {gptRuntime}s\\n\")\n",
        "        f.write(f\"MRR: {MRR}\\n\")\n",
        "\n",
        "    accuracyPerNumArg = []\n",
        "    row = []\n",
        "    row.append(\"Number of params\")\n",
        "    row.append(\"Percentage of distribution\")\n",
        "    for k in tops:\n",
        "        row.append(f\"GPT's top-{k} accuracy\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} precision\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} recall\")\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    unique, counts = np.unique(dataFrame['ArgType'], return_counts=True)\n",
        "    counts = counts / counts.sum()\n",
        "    argTypeDict = defaultdict(float)\n",
        "    for i in range(len(unique)):\n",
        "        argTypeDict[unique[i]] = counts[i]\n",
        "\n",
        "    for i in range(len(expressionTypes)):\n",
        "        argType = expressionTypes[i]\n",
        "        row = []\n",
        "        row.append(argType)\n",
        "        row.append(argTypeDict[i] * 100)\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTOverallTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTActualTop{k}{argType}\"]))\n",
        "        accuracyPerNumArg.append(row)\n",
        "\n",
        "    row = []\n",
        "    row.append(\"all\")\n",
        "    row.append(\"100\")\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTOverallTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTActualTop{k}\"]))\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    with open(f'results/{repo_dir}arg_rec_{model}.csv', 'w') as f:\n",
        "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        for row in accuracyPerNumArg:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    with open(f'results/{repo_dir}arg_rec_{model}_log.txt', 'a') as f:\n",
        "        for k in tops:\n",
        "            correctTestsCount = np.sum(dataFrame[f\"GPTActualTop{k}\"])\n",
        "            f.write(f\"Target showed up in top {k} recommendations in {correctTestsCount} tests.\\n\")\n",
        "\n",
        "printTestResult()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4neBPx5Q4ecd"
      },
      "source": [
        "for text_file in os.listdir('results/' + repo_dir):\n",
        "    shutil.copyfile('results/' + repo_dir + text_file, main_path + 'results/' + repo_dir + text_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gb-Qex8wPUXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}