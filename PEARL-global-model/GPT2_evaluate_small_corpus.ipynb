{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_evaluate_small_corpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "792c8b50623841f886f8f24bae56fdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_574d2af06b7949dfb1bfaa34845d8f0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3c487f5d247422d95903bc9f5edb89a",
              "IPY_MODEL_f6d1e3fa29a544f78379ed0149be5d3c",
              "IPY_MODEL_842ed8c583b9427cba8ef208ee19e4f3"
            ]
          }
        },
        "574d2af06b7949dfb1bfaa34845d8f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3c487f5d247422d95903bc9f5edb89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea31db30de4a449e90a3c6fbbcec61ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8192aba951e44e3bb4d1f7bb9af9abb3"
          }
        },
        "f6d1e3fa29a544f78379ed0149be5d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_589a344f6dc747029cacd42dbf60ec6c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9554831bd8c74a32b44cfef0e7575067"
          }
        },
        "842ed8c583b9427cba8ef208ee19e4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9bd8c202dfef475a9ed6f82ab982a09a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [04:10&lt;00:00, 84.97s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7faf7d8dea2046f4bb5707b1b191b051"
          }
        },
        "ea31db30de4a449e90a3c6fbbcec61ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8192aba951e44e3bb4d1f7bb9af9abb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "589a344f6dc747029cacd42dbf60ec6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9554831bd8c74a32b44cfef0e7575067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bd8c202dfef475a9ed6f82ab982a09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7faf7d8dea2046f4bb5707b1b191b051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq4iC6vUAHc",
        "outputId": "a562c846-aa54-4d36-e078-8a8f3b48296e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "source": [
        "main_path = \"drive/MyDrive/AI/GPT/\"\n",
        "repo_dir = \"eclipse_netbeans/\"\n",
        "project_name = \"eclipse\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir\n",
        "model = \"gpt2\"\n",
        "pred_path = main_path + \"predictions/\" + repo_dir + \"gpt2/\"\n",
        "result_path = main_path + \"results/\" + repo_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName, foldId):\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests_fold{foldId}.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ],
      "metadata": {
        "id": "jKjZVhFuX-DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    test['methodInvocClassQualifiedName'] = this['methodInvocClassQualifiedName']\n",
        "    return test"
      ],
      "metadata": {
        "id": "syDJAophYIkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ],
      "metadata": {
        "id": "PpFtWc-GYL0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "source": [
        "import os\n",
        "\n",
        "def readPredictions(projectName, foldId):\n",
        "    predictions = []\n",
        "    if os.path.isfile(f\"{pred_path}{projectName}_ArgRecs_fold{foldId}.txt\"):\n",
        "        filePath = f\"{pred_path}{projectName}_ArgRecs_fold{foldId}.txt\"\n",
        "\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN-nKkwg6Va8"
      },
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and result == \"<LAMBDA>\":\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YpySsDlz_m"
      },
      "source": [
        "def canAcceptResult(test, result):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    result = preprocess(result)\n",
        "    if '{' in result:\n",
        "        result = result[:result.index('{')].rstrip()\n",
        "    if result.find('(') > 0:\n",
        "        result = preprocessor.normalize_method_invocation(result)\n",
        "    \n",
        "    if matchesArg(expectedLex, result):\n",
        "        return True\n",
        "\n",
        "    alternateLex = None\n",
        "    if 'methodAccessLex' in test:\n",
        "        alternateLex = test['methodAccessLex']\n",
        "    if 'objectCreationLex' in test:\n",
        "        alternateLex = test['objectCreationLex']\n",
        "    if alternateLex is not None and matchesArg(alternateLex, result):\n",
        "        return True\n",
        "\n",
        "    if 'staticMemberAccessLex' in test:\n",
        "        if matchesArg(test['staticMemberAccessLex'], result):\n",
        "            return True\n",
        "            \n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUqA-mJ36SFm"
      },
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "os.makedirs('results/' + project_name, exist_ok=True)\n",
        "os.makedirs('logs/' + project_name, exist_ok=True)"
      ],
      "metadata": {
        "id": "L82NnK3naslK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b96CurBBe5ET"
      },
      "source": [
        "dataFrame = defaultdict(list)\n",
        "\n",
        "def updateTopKResult(test, results, k, adequateGeneratedCandidate, doPrintIncorrectPrediction, projectName):\n",
        "    isOverallCorrectTopK = False\n",
        "    for i in range(min(k, len(results))):\n",
        "        if canAcceptResult(test, results[i]):\n",
        "            isOverallCorrectTopK = True\n",
        "            break\n",
        "\n",
        "    argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else 'null'\n",
        "    if isOverallCorrectTopK:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(1)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(1)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(1)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(1)\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(1)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(1)\n",
        "    else:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(0)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(0)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(0)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(0)\n",
        "            if doPrintIncorrectPrediction:\n",
        "                outputFileName = f\"{projectName}_incorrect_ArgRecTests_top_{k}.txt\"\n",
        "                with open(f'logs/{repo_dir}{outputFileName}', \"a\") as f:\n",
        "                    f.write(json.dumps(test['argRecTestList'][0]) + '\\n')\n",
        "                    f.write('Predictions: ' + str(results) + '\\n')\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(0)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "792c8b50623841f886f8f24bae56fdf9",
            "574d2af06b7949dfb1bfaa34845d8f0e",
            "b3c487f5d247422d95903bc9f5edb89a",
            "f6d1e3fa29a544f78379ed0149be5d3c",
            "842ed8c583b9427cba8ef208ee19e4f3",
            "ea31db30de4a449e90a3c6fbbcec61ca",
            "8192aba951e44e3bb4d1f7bb9af9abb3",
            "589a344f6dc747029cacd42dbf60ec6c",
            "9554831bd8c74a32b44cfef0e7575067",
            "9bd8c202dfef475a9ed6f82ab982a09a",
            "7faf7d8dea2046f4bb5707b1b191b051"
          ]
        },
        "id": "DCzjVqmp-3Eg",
        "outputId": "845dc2fc-86cd-4a5b-94a3-8f94b9367a6d"
      },
      "source": [
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "COMPOUND_CONSIDERED = False\n",
        "TEST_APIS = [\n",
        "            #  \"org.eclipse.swt\",\n",
        "            #  \"java.awt\",\n",
        "            #  \"javax.swing\",\n",
        "]\n",
        "if len(TEST_APIS) > 0:\n",
        "    model += \"__lib\"\n",
        "TEST_LOCAL_ARG = None\n",
        "if TEST_LOCAL_ARG is not None:\n",
        "    if TEST_LOCAL_ARG:\n",
        "        model += \"__local_arg\"\n",
        "    else:\n",
        "        model += \"__not_local_arg\"\n",
        "\n",
        "rank_list = []\n",
        "\n",
        "fold_list = []\n",
        "for testFile in os.listdir(pred_path):\n",
        "    if testFile.startswith(f\"{project_name}_ArgRecs_fold\"):\n",
        "        fold_id = testFile[testFile.find(\"fold\")+4:][0]\n",
        "        fold_list.append(fold_id)\n",
        "\n",
        "for fold_id in tqdm(fold_list):\n",
        "    tests = allTestsToSingleArgRecTest(readTests(project_name, fold_id))\n",
        "    predictions = readPredictions(project_name, fold_id)\n",
        "    for i in range(min(len(tests), len(predictions))):\n",
        "        test = tests[i]\n",
        "        prediction_detail = predictions[i]\n",
        "        dataFrame['Tested'].append(1)\n",
        "\n",
        "        if test['numArg'] == 0:\n",
        "            continue\n",
        "        if len(TEST_APIS) > 0:\n",
        "            is_target = False\n",
        "            for target_api in TEST_APIS:\n",
        "                if test['methodInvocClassQualifiedName'].startswith(target_api + '.'):\n",
        "                    is_target = True\n",
        "            if not is_target:\n",
        "                continue\n",
        "        if TEST_LOCAL_ARG is not None:\n",
        "            if not predictions[i]['sufficient_candidates']:\n",
        "                continue\n",
        "            is_local_arg = False\n",
        "            for j in range(len(test['next_lex'][0])):\n",
        "                for k in range(len(test['next_lex'][0][j])):\n",
        "                    candidate = test['next_lex'][0][j][k]\n",
        "                    if candidate == test['expected_lex']:\n",
        "                        candidate_locality = test['argRecTestList'][0]['candidates_locality'][j][k]\n",
        "                        if candidate_locality >= 4:\n",
        "                            is_local_arg = True\n",
        "                            break\n",
        "                if is_local_arg:\n",
        "                    break\n",
        "            if TEST_LOCAL_ARG != is_local_arg:\n",
        "                continue\n",
        "\n",
        "        dataFrame['Predicted'].append(1)\n",
        "        if not test['ignored']:\n",
        "            dataFrame['Predicted supported'].append(1)\n",
        "        \n",
        "        gptResults = prediction_detail['predictions']\n",
        "        if not COMPOUND_CONSIDERED:\n",
        "            if '<COMPOUND>' in gptResults:\n",
        "                gptResults.remove('<COMPOUND>')\n",
        "\n",
        "        rank = -1\n",
        "        for k in range(min(10, len(gptResults))):\n",
        "            if canAcceptResult(test, gptResults[k]):\n",
        "                rank = k\n",
        "        rank_list.append(rank)\n",
        "\n",
        "        for k in tops:\n",
        "            updateTopKResult(test, gptResults, k, prediction_detail['sufficient_candidates'], False, project_name)\n",
        "\n",
        "        dataFrame[\"GPT's runtime\"].append(prediction_detail['runtime'])\n",
        "        argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else None\n",
        "        if argType is not None:\n",
        "            dataFrame[\"ArgType\"].append(expressionTypeDict[argType])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "792c8b50623841f886f8f24bae56fdf9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MRR = 0\n",
        "for rank in rank_list:\n",
        "    if rank < 0:\n",
        "        MRR += 0\n",
        "    else:\n",
        "        MRR += 1/(rank + 1)\n",
        "MRR /= len(rank_list)\n",
        "print(MRR)"
      ],
      "metadata": {
        "id": "bIIctu-A4RSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80653bb9-1d79-487e-c91e-841f4ed283b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4322325334651104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtmIUfQaNrr-"
      },
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('logs/dataframe.pkl', 'wb') as f:\n",
        "#     pickle.dump(dataFrame, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXCB5cWdvYKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99cafe9-5865-4fd4-f625-fa37dd73fc72"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def printTestResult():\n",
        "    with open(f'results/{project_name}/arg_rec_{model}_log.txt', 'w') as f:\n",
        "        f.write(f\"Ran {len(dataFrame['Tested'])} tests successfully.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted'])} tests.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted supported'])} tests that were supported.\\n\")\n",
        "        f.write(f\"Skipped {len(dataFrame['Tested']) - len(dataFrame['Predicted'])} tests. They were not taken into account during evaluation.\\n\")\n",
        "        gptRuntime = np.mean(dataFrame[\"GPT's runtime\"])\n",
        "        f.write(f\"GPT's runtime: {gptRuntime}s\\n\")\n",
        "        f.write(f\"MRR: {MRR}\\n\")\n",
        "\n",
        "    accuracyPerNumArg = []\n",
        "    row = []\n",
        "    row.append(\"Number of params\")\n",
        "    row.append(\"Percentage of distribution\")\n",
        "    for k in tops:\n",
        "        row.append(f\"GPT's top-{k} accuracy\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} precision\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} recall\")\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    unique, counts = np.unique(dataFrame['ArgType'], return_counts=True)\n",
        "    counts = counts / counts.sum()\n",
        "    argTypeDict = defaultdict(float)\n",
        "    for i in range(len(unique)):\n",
        "        argTypeDict[unique[i]] = counts[i]\n",
        "\n",
        "    for i in range(len(expressionTypes)):\n",
        "        argType = expressionTypes[i]\n",
        "        row = []\n",
        "        row.append(argType)\n",
        "        row.append(argTypeDict[i] * 100)\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTOverallTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTActualTop{k}{argType}\"]))\n",
        "        accuracyPerNumArg.append(row)\n",
        "\n",
        "    row = []\n",
        "    row.append(\"all\")\n",
        "    row.append(\"100\")\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTOverallTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTActualTop{k}\"]))\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    with open(f'results/{project_name}/arg_rec_{model}.csv', 'w') as f:\n",
        "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        for row in accuracyPerNumArg:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    with open(f'results/{project_name}/arg_rec_{model}_log.txt', 'a') as f:\n",
        "        for k in tops:\n",
        "            correctTestsCount = np.sum(dataFrame[f\"GPTActualTop{k}\"])\n",
        "            f.write(f\"Target showed up in top {k} recommendations in {correctTestsCount} tests.\\n\")\n",
        "\n",
        "printTestResult()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4neBPx5Q4ecd"
      },
      "source": [
        "for text_file in os.listdir('results/' + project_name):\n",
        "    shutil.copyfile('results/' + project_name + '/' + text_file, main_path + 'results/' + project_name + '/' + text_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gb-Qex8wPUXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}