{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKYC1d_h-1T7"
      },
      "source": [
        "## CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvgme1E3IJfB",
        "outputId": "3ee0cd75-6f38-4475-b906-f944d197b815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-dev libnvinfer-plugin7 libnvinfer7\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 143 MB of archives.\n",
            "After this operation, 520 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer7 7.0.0-1+cuda10.0 [69.6 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-dev 7.0.0-1+cuda10.0 [71.6 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-plugin7 7.0.0-1+cuda10.0 [2,109 kB]\n",
            "Fetched 143 MB in 2s (60.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer7.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvinfer7_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer7 (7.0.0-1+cuda10.0) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../libnvinfer-dev_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (7.0.0-1+cuda10.0) ...\n",
            "Selecting previously unselected package libnvinfer-plugin7.\n",
            "Preparing to unpack .../libnvinfer-plugin7_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin7 (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer7 (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer-dev (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer-plugin7 (7.0.0-1+cuda10.0) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y --no-install-recommends libnvinfer7=7.0.0-1+cuda10.0 \\\n",
        "    libnvinfer-dev=7.0.0-1+cuda10.0 \\\n",
        "    libnvinfer-plugin7=7.0.0-1+cuda10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Installing necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "db52bb05-fcf9-45bc-9679-e500a8ca7369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 30 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.21.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.13.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.0.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=35993739a89f5d44b06fe54a7915ca5da378304faaa74bb1e976e3c6b3d543e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow-gpu\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 tensorflow-gpu-1.15.2\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "#!pip install tensorflow==1.15.2\n",
        "!pip install tensorflow-gpu==1.15.2\n",
        "!pip install -q gpt-2-simple==0.7.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQFo2PINvblo",
        "outputId": "3573cb5d-a54b-42d8-f12f-155d9bb44cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## Verifying GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmTooTW3osf",
        "outputId": "d3c34e4b-16e3-4d35-cb62-c6c5cde1c464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec 27 08:15:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgvl-BJ7I4H",
        "outputId": "a862ccb9-7269-4cae-e095-f16638cf410b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5661350125407305619\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15334983247480132462\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 8196194454160575818\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15964005991\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5373410271582225779\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxKEhGLhQ2H0",
        "outputId": "5fdf8dc0-e0fe-42d9-914b-b0aeb8079163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "outputs": [],
      "source": [
        "main_path = \"drive/MyDrive/shared/GPT/\"\n",
        "repo_dir = \"eclipse_netbeans/\"\n",
        "project_name = \"eclipse\"\n",
        "fold_id = 0\n",
        "setting = \"dynamic\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name=f'fold{fold_id}_excluded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Ranking candidates using GPT-2\n",
        "before combining additional features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "outputs": [],
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlD93fzPsmo_"
      },
      "outputs": [],
      "source": [
        "from gpt.gpt_manager import GPTManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVeJYgBAXT3h"
      },
      "outputs": [],
      "source": [
        "gpt_manager = GPTManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cb7co1DZthy",
        "outputId": "4d0310e6-ef4e-49d5-ac94-d662ed934170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "244 end tokens.\n",
            "Loading model checkpoint/run1/model-50000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-50000\n",
            "WARNING:tensorflow:From /content/gpt/sample.py:33: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ],
      "source": [
        "gpt_manager.java_model = gpt_manager.load_model('checkpoint/', 'run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName, foldId):\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests_fold{foldId}.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGJJ4ZQn6IaN"
      },
      "outputs": [],
      "source": [
        "def allTestsToNonZeroArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            tests.append(test)\n",
        "    return tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo5f8WZvx2zz"
      },
      "outputs": [],
      "source": [
        "def readFilterPreds(projectName, foldId):\n",
        "    filters_predictions = []\n",
        "    with open(f\"{main_path}predictions/{repo_dir}filter_{setting}/{projectName}/fold{foldId}/{projectName}_prediction_detail_flute_sequence.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines:\n",
        "            if len(line) > 0:\n",
        "                filters_predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return filters_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VErRoqgPz1O9"
      },
      "outputs": [],
      "source": [
        "def use_gpt_to_predict(candidate):\n",
        "    # COMPOUND\n",
        "    if candidate == \"<COMPOUND>\":\n",
        "        return True\n",
        "\n",
        "    # LAMBDA\n",
        "    if candidate == \"<LAMBDA>\":\n",
        "        return True\n",
        "\n",
        "    # NULL_LIT\n",
        "    if candidate == \"null\":\n",
        "        return False\n",
        "\n",
        "    # BOOL_LIT\n",
        "    if candidate == \"true\" or candidate == \"false\":\n",
        "        return False\n",
        "\n",
        "    # NUM_LIT, CHAR_LIT\n",
        "    if candidate == \"0\":\n",
        "        return False\n",
        "\n",
        "    # THIS, SUPER\n",
        "    if candidate == \"this\" or candidate == \"super\":\n",
        "        #return main_score\n",
        "        return False\n",
        "\n",
        "    # CAST\n",
        "    if candidate[0] == \"(\":\n",
        "        return False\n",
        "\n",
        "    # M_REF\n",
        "    if \"::\" in candidate:\n",
        "        return True\n",
        "\n",
        "    # NAME, TYPE_LIT, LAMBDA\n",
        "    if (candidate[-1] not in [\"(\", \"\\\"\", \"[\", \"]\"]) and (candidate != \"null\"):\n",
        "        if candidate.find('.') <= 0 or \"->\" in candidate:\n",
        "            return True\n",
        "\n",
        "    # FIELD_ACCESS\n",
        "    if (candidate[-1] not in [\"(\", \"\\\"\", \"[\", \"]\"]) and (candidate != \"null\"):\n",
        "        if candidate.find('.') > 0 and \"->\" not in candidate:\n",
        "            return False\n",
        "\n",
        "    # METHOD_INVOC\n",
        "    if candidate[-1] == \"(\" and not candidate.startswith(\"new \"):\n",
        "        return False\n",
        "\n",
        "    # OBJECT_CREATION\n",
        "    if candidate[-1] == \"(\" and candidate.startswith(\"new \"):\n",
        "        #return main_score\n",
        "        return False\n",
        "\n",
        "    # ARRAY_ACCESS, ARR_CREATION\n",
        "    if candidate[-1] in [\"[\", \"]\"]:\n",
        "        return True\n",
        "\n",
        "    # STRING_LIT\n",
        "    if candidate[-1] == \"\\\"\":\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAAU33_c1K_8"
      },
      "outputs": [],
      "source": [
        "PREDICTION_COUNT_LIM = 20\n",
        "\n",
        "def allTestsToFilteredTest(oneArgTests, filters_predictions):\n",
        "    for i in range(len(oneArgTests)):\n",
        "        #prediction_set = set(filters_predictions[i]['predictions'][:PREDICTION_COUNT_LIM])\n",
        "        prediction_set = set()\n",
        "        for candidate in filters_predictions[i]['predictions'][:PREDICTION_COUNT_LIM]:\n",
        "            if use_gpt_to_predict(candidate):\n",
        "                prediction_set.add(candidate)\n",
        "                if len(prediction_set) == PREDICTION_COUNT_LIM:\n",
        "                    break\n",
        "        for j in range(len(oneArgTests[i]['next_lex'])):\n",
        "            lex_candidate_list = []\n",
        "            for candidate in oneArgTests[i]['next_lex'][j]:\n",
        "                if candidate in prediction_set:\n",
        "                    lex_candidate_list.append(candidate)\n",
        "                    prediction_set.remove(candidate)\n",
        "            oneArgTests[i]['next_lex'][j] = lex_candidate_list\n",
        "\n",
        "        # TODO: Add proper excode candidates\n",
        "        if len(prediction_set) > 0:\n",
        "            oneArgTests[i]['next_excode'].append(oneArgTests[i]['expected_excode'])\n",
        "            oneArgTests[i]['next_lex'].append(list(prediction_set))\n",
        "        excode_candidate_list = []\n",
        "        lex_candidate_list = []\n",
        "        for j in range(len(oneArgTests[i]['next_lex'])):\n",
        "            if len(oneArgTests[i]['next_lex'][j]) > 0:\n",
        "                excode_candidate_list.append(oneArgTests[i]['next_excode'][j])\n",
        "                lex_candidate_list.append(oneArgTests[i]['next_lex'][j])\n",
        "        oneArgTests[i]['next_excode'] = excode_candidate_list\n",
        "        oneArgTests[i]['next_lex'] = lex_candidate_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23kjL0oW1Cqa"
      },
      "outputs": [],
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d67IQ8C91IMc"
      },
      "outputs": [],
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        #if test['argPos'] > 0:\n",
        "        if True:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "outputs": [],
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDWp3yqL7Ka1"
      },
      "outputs": [],
      "source": [
        "def preprocess_filter(candidate):\n",
        "    candidate = preprocessor.empty_string_literal(candidate)\n",
        "    if \"{\" in candidate:\n",
        "        candidate = candidate[:candidate.index(\"{\")].rstrip()\n",
        "    if \"]\" in candidate:\n",
        "        candidate = preprocessor.remove_array_access_index(candidate)\n",
        "    if \"(\" in candidate and candidate.index(\"(\") > 0:\n",
        "        candidate = preprocessor.normalize_method_invocation(candidate)\n",
        "\n",
        "    # Lambda expression\n",
        "    if \"->\" in candidate:\n",
        "        candidate = \"x -> {}\"\n",
        "\n",
        "    # Exclude candidates starting with this if they are redundant\n",
        "    if candidate.startswith(\"this.\"):\n",
        "        candidate = candidate[5:]\n",
        "\n",
        "    return candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlN8Rgy77Osj"
      },
      "outputs": [],
      "source": [
        "def preprocess_all_filter_preds(filters_predictions):  \n",
        "  for i in range(len(filters_predictions)):\n",
        "      for j in range(len(filters_predictions[i]['predictions'])):\n",
        "          filters_predictions[i]['predictions'][j] = preprocess_filter(filters_predictions[i]['predictions'][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6HyRqvGa6fY"
      },
      "outputs": [],
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaWz9QQsAXoV"
      },
      "outputs": [],
      "source": [
        "def canAcceptGeneratedLexes(test):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    for group in test['next_lex']:\n",
        "        for candidate in group:\n",
        "            candidate = preprocess(candidate)\n",
        "            if matchesArg(expectedLex, candidate):\n",
        "                return True\n",
        "\n",
        "            alternateLex = None\n",
        "            if 'methodAccessLex' in test:\n",
        "                alternateLex = test['methodAccessLex']\n",
        "            if 'objectCreationLex' in test:\n",
        "                alternateLex = test['objectCreationLex']\n",
        "            if alternateLex is not None and matchesArg(alternateLex, candidate):\n",
        "                return True\n",
        "\n",
        "            if 'staticMemberAccessLex' in test:\n",
        "                if matchesArg(test['staticMemberAccessLex'], candidate):\n",
        "                    return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUdqQfcBbA0A"
      },
      "outputs": [],
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqn5PBv8c19j"
      },
      "outputs": [],
      "source": [
        "def validateTest(test):\n",
        "    if not test['ignored']:\n",
        "        adequateGeneratedLex = False\n",
        "        if canAcceptGeneratedLexes(test):\n",
        "            adequateGeneratedLex = True;\n",
        "        if adequateGeneratedLex:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzjVqmp-3Eg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "os.makedirs(f'predictions/{repo_dir}flute_{setting}/', exist_ok=True)\n",
        "\n",
        "predictionFilePath = f'predictions/{repo_dir}flute_{setting}/{project_name}_ArgRecs_fold{fold_id}.txt'\n",
        "prediction_details = []\n",
        "tests = readTests(project_name, fold_id)\n",
        "filter_preds = readFilterPreds(project_name, fold_id)\n",
        "allTestsToFilteredTest(tests, filter_preds)\n",
        "tests = allTestsToSingleArgRecTest(tests)\n",
        "preprocess_all_filter_preds(filter_preds)\n",
        "\n",
        "assert len(tests) == len(filter_preds), \"Tests not matched!\"\n",
        "for i in range(len(tests)):\n",
        "    assert tests[i]['expected_lex'][:3] == filter_preds[i]['answer'][:3], \"Tests not matched!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7d8744012f7c47b6879c150a7c78a2e0",
            "01f4d1c6780743d59d71331365c2dc30",
            "7f8fb2b8ac0b4b0a910fb2bf2f42d395",
            "0477d319aea04d0d9557f76da68f77cd",
            "284a6f45abbf43d9a153358ce0c3c6e1",
            "90118ae65d8c4397861b76ca77111242",
            "072f55f15c7b438d9d0d5136bee0cab9",
            "f0071b0c1c9f40989e8d6b16ece69ee4",
            "ad003e618d4e424f82e9552f9c5231b3",
            "5de73259141b4b8d8a07ace07dd00351",
            "acf62a46dc8a4b2cb73c43077eedde78"
          ]
        },
        "id": "0v80HNN6luiQ",
        "outputId": "ae181181-cbed-443d-9da8-0b7ff45f920e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d8744012f7c47b6879c150a7c78a2e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/400 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in tqdm(range(len(tests))):\n",
        "    test = tests[i]\n",
        "    if test['numArg'] == 0:\n",
        "        continue\n",
        "    \n",
        "    response = gpt_manager.predict_param_using_lex(test)\n",
        "    gptResults = response['result']\n",
        "    gptScores = response['scores']\n",
        "\n",
        "    prediction_dict = {}\n",
        "    for j in range(len(filter_preds[i]['predictions']) - 1, -1, -1):\n",
        "        prediction_dict[filter_preds[i]['predictions'][j]] = filter_preds[i]['lexModelScores'][j]\n",
        "        # # CAST\n",
        "        # if filter_preds[i]['predictions'][j][0] == '(':\n",
        "        #     prediction_dict[filter_preds[i]['predictions'][j]] += np.log(0.1)\n",
        "        if prediction_dict[filter_preds[i]['predictions'][j]] > 0:\n",
        "            prediction_dict[filter_preds[i]['predictions'][j]] = -64\n",
        "\n",
        "    for j in range(len(gptResults)):\n",
        "        prediction_dict[gptResults[j]] = gptScores[j]\n",
        "    combinedResults = sorted(list(set(filter_preds[i]['predictions'])), key=lambda x: -prediction_dict[x])\n",
        "\n",
        "    prediction_detail = {}\n",
        "    prediction_detail['test_id'] = test['id']\n",
        "    prediction_detail['predictions'] = combinedResults\n",
        "    prediction_detail['scores'] = [prediction_dict[x] for x in combinedResults]\n",
        "    prediction_detail['answer'] = test['expected_lex']\n",
        "    prediction_detail['arg_type'] = test['argRecTestList'][0]['argType']\n",
        "    prediction_detail['runtime'] = response['runtime']\n",
        "    prediction_detail['ignored'] = test['ignored']\n",
        "    prediction_detail['sufficient_candidates'] = validateTest(test)\n",
        "    prediction_details.append(prediction_detail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5TiVQ4iCHaD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a57d6ede-c29a-4804-954e-bee8b5275fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/MyDrive/shared/GPT/predictions/eclipse_netbeans/flute_dynamic/eclipse_ArgRecs_fold2.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "with open(predictionFilePath, \"w\") as f:\n",
        "    for prediction_detail in prediction_details:\n",
        "        f.write(json.dumps(prediction_detail) + '\\n')\n",
        "\n",
        "shutil.copyfile(predictionFilePath, main_path + predictionFilePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LV8HifMiou5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "PEARL_predict_small_corpus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d8744012f7c47b6879c150a7c78a2e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_01f4d1c6780743d59d71331365c2dc30",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f8fb2b8ac0b4b0a910fb2bf2f42d395",
              "IPY_MODEL_0477d319aea04d0d9557f76da68f77cd",
              "IPY_MODEL_284a6f45abbf43d9a153358ce0c3c6e1"
            ]
          }
        },
        "01f4d1c6780743d59d71331365c2dc30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f8fb2b8ac0b4b0a910fb2bf2f42d395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90118ae65d8c4397861b76ca77111242",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_072f55f15c7b438d9d0d5136bee0cab9"
          }
        },
        "0477d319aea04d0d9557f76da68f77cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0071b0c1c9f40989e8d6b16ece69ee4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad003e618d4e424f82e9552f9c5231b3"
          }
        },
        "284a6f45abbf43d9a153358ce0c3c6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5de73259141b4b8d8a07ace07dd00351",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 400/400 [01:44&lt;00:00,  2.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acf62a46dc8a4b2cb73c43077eedde78"
          }
        },
        "90118ae65d8c4397861b76ca77111242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "072f55f15c7b438d9d0d5136bee0cab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0071b0c1c9f40989e8d6b16ece69ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad003e618d4e424f82e9552f9c5231b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5de73259141b4b8d8a07ace07dd00351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acf62a46dc8a4b2cb73c43077eedde78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}