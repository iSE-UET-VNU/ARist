{"cells":[{"cell_type":"markdown","source":["# LSTM model testing\n","Change the paths, `project` and `train_len` and run the notebook.\n"],"metadata":{"id":"WbNZAJbD2nE0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9567,"status":"ok","timestamp":1645365768284,"user":{"displayName":"Son Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00583459623039791697"},"user_tz":-420},"id":"gWSy_jqdB-TH","outputId":"8e5d5d87-3124-4459-fb6f-ea57b8b97baa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wordsegment\n","  Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n","\u001b[K     |████████████████████████████████| 4.8 MB 4.7 MB/s \n","\u001b[?25hCollecting np\n","  Downloading np-1.0.2.tar.gz (7.4 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Building wheels for collected packages: np\n","  Building wheel for np (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for np: filename=np-1.0.2-py3-none-any.whl size=13676 sha256=bde195a0ac65d560a0721200eb2f23cbba445028f54d4bb1465814e90aaf0643\n","  Stored in directory: /root/.cache/pip/wheels/8d/31/5b/f3f27c678f2b3ad7e29903ed09bb7446717fd4c8b35f53973a\n","Successfully built np\n","Installing collected packages: wordsegment, np\n","Successfully installed np-1.0.2 wordsegment-1.3.1\n"]}],"source":["!pip install wordsegment np nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uc7nrRz_cunT"},"outputs":[],"source":["import json\n","from time import perf_counter\n","from pickle import load\n","from keras.models import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foAHShyld5rE"},"outputs":[],"source":["project = 'netbeans'\n","fold = '9'\n","top_k = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzfStuLaBFOp"},"outputs":[],"source":["from keras.backend import reshape\n","import re\n","import np\n","import wordsegment\n","from nltk import wordpunct_tokenize\n","\n","wordsegment.load()\n","\n","REG = r\"(.+?)([A-Z])\"\n","\n","\n","def splitCase(match):\n","    return match.group(1).lower() + \"/\" + match.group(2).lower()\n","\n","\n","def tokenize(word):\n","    if (len(word) == 0):\n","        return []\n","\n","    camelCases = []\n","\n","    # Do not predict snake_cases\n","    # if (word.find(\"_\") != -1 or len(word) == 0):\n","    # return []\n","\n","    ppWord = '%s' % word\n","\n","    TAREG = re.compile(\"[<,>?\\[\\](){}&.|_=]\")\n","    for match in re.finditer(\"[A-Z][A-Z\\d]+\", ppWord):\n","        result = \"\"\n","        s = match.start()\n","        e = match.end()\n","        if (e == len(ppWord) or bool(TAREG.match(ppWord[e]))):\n","            # CLASS => Class\n","            result = ppWord[s:e][1:].lower()\n","            result = ppWord[s:e][0] + result\n","        else:\n","            # CLASS => ClasS\n","            result = ppWord[s:e][1:-1].lower()\n","            result = ppWord[s:e][0] + result + ppWord[s:e][-1]\n","\n","        ppWord = ppWord[:s] + result + ppWord[e:]\n","\n","    # Split ...\n","    words = ppWord.split(\"...\")\n","\n","    # Split type argument character\n","    for w in words:\n","        tmpWord = \"\"\n","\n","        for char in w:\n","            if (bool(TAREG.match(char))):\n","                camelCases.append(tmpWord)\n","                camelCases.append(char)\n","                tmpWord = \"\"\n","            else:\n","                tmpWord += char\n","        if (len(tmpWord) > 0):\n","            camelCases.append(tmpWord)\n","\n","        camelCases.append(\"...\")\n","\n","    camelCases.pop()\n","\n","    split_cases = [re.sub(REG, splitCase, w, 0).lower() for w in camelCases]\n","    words = [re.split(\"/\", w) for w in split_cases]\n","\n","    words = np.concatenate(words).tolist()\n","\n","    result = []\n","    for ele in words:\n","        if (bool(TAREG.match(ele))):\n","            result.append([ele])\n","        else:\n","            result.append(wordsegment.segment(ele))\n","\n","    return np.concatenate(result).tolist()\n","\n","\n","def remove_comments(string):\n","    pattern = r\"(\\\".*?\\\"|\\'.*?\\')|(/\\*.*?\\*/|//[^\\r\\n]*$)\"\n","    # first group captures quoted strings (double or single)\n","    # second group captures comments (//single-line or /* multi-line */)\n","    regex = re.compile(pattern, re.MULTILINE|re.DOTALL)\n","    def _replacer(match):\n","        # if the 2nd group (capturing comments) is not None,\n","        # it means we have captured a non-quoted (real) comment string.\n","        if match.group(2) is not None:\n","            return \"\" # so we will return empty to remove the comment\n","        else: # otherwise, we will return the 1st group\n","            return match.group(1) # captured quoted-string\n","    return regex.sub(_replacer, string)\n","\n","\n","# def tokenize_subtoken(txt):\n","#     src = remove_comments(txt)\n","#     src = wordpunct_tokenize(txt)\n","#     res = []\n","#     for token in src:\n","#         if not token[0].isalnum():\n","#             for p in token:\n","#                 res.append(p)\n","#         else:\n","#             res += tokenize(token)\n","#     return reshape\n","\n","\n","def tokenize_fulltoken(txt):\n","    src = remove_comments(txt)\n","    src = wordpunct_tokenize(txt)\n","    res = []\n","    for token in src:\n","        if not token[0].isalnum():\n","            for p in token:\n","                res.append(p)\n","        else:\n","            res.append(token)\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0boEBaOoK3F"},"outputs":[],"source":["import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","def prepare_sentence(seq, train_len, start_pos):\n","    # Pads seq and slides windows\n","    x = []\n","    y = []\n","    for i in range(start_pos, len(seq)):\n","        x_padded = pad_sequences([seq[:i]],\n","                                 maxlen=train_len - 1,\n","                                 padding='pre')[0]  # Pads before each sequence\n","        x.append(x_padded)\n","        y.append(seq[i])\n","    return x, y\n","\n","\n","def prepare_sentences(context, sentences, train_len, start_pos):\n","    x_test_all = []\n","    y_test_all = []\n","    sentence_len = []\n","    for sentence in sentences:\n","        x_test, y_test = prepare_sentence(context + sentence, train_len, start_pos)\n","        x_test_all += x_test\n","        y_test_all += y_test\n","        sentence_len += [len(x_test)]\n","    return np.array(x_test_all), np.array(y_test_all), sentence_len\n","\n","\n","def predict(model, x, **kwargs):\n","    model_input = [x]\n","    return model.predict(model_input, workers=4, use_multiprocessing=True, batch_size=200)\n","\n","\n","def evaluate(p_pred, y_test, sentence_len, **kwargs):\n","    log_p_sentence = [0] * len(sentence_len)\n","    x_test_id = 0\n","    accumulate_len = 0\n","\n","    for i, prob in enumerate(p_pred):\n","        # word = vocab_inv[y_test[i] + 1]  # Index 0 from vocab is reserved to <PAD>\n","        # history = ''.join([vocab_inv[w] for w in x_test[i, :] if w != 0])\n","        if i - accumulate_len == sentence_len[x_test_id]:\n","            accumulate_len += sentence_len[x_test_id]\n","            x_test_id += 1\n","        prob_word = prob[y_test[i]]\n","        log_p_sentence[x_test_id] += np.log(prob_word)\n","        # print('P(w={}|h={})={}'.format(word, history, prob_word))\n","        # print('Prob. sentence: {}'.format(log_p_sentence))\n","    return log_p_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yl-ycOv8RZuu"},"outputs":[],"source":["def java_tokenize_sentences(lexes, tokenizer, to_sequence=True):\n","    text_sequences = []\n","    for lex in lexes:\n","        text_sequences.append(tokenize_fulltoken(lex))\n","    sequences = tokenizer.texts_to_sequences(text_sequences)\n","    return sequences\n","\n","def java_tokenize_take_last(lines, tokenizer, train_len):\n","    all_tokens = []\n","    for line in lines:\n","        all_tokens += tokenize_fulltoken(line)\n","    # print(all_tokens)\n","    seq = all_tokens[max(len(all_tokens) - train_len, 0):len(all_tokens)]\n","    sequences = tokenizer.texts_to_sequences([seq])\n","    return sequences[0]\n","  \n","def select_top_candidates(java_context, data, start_time):\n","    sorted_scores = sorted(java_context, key=lambda x: -x[0])[:top_k]\n","    prediction_detail = {}\n","    predictions = []\n","    for score in sorted_scores:\n","        predictions.append(score[1])\n","    prediction_detail['lexModelScores'] = list(map(lambda x: x[0], sorted_scores))\n","    prediction_detail['predictions'] = list(map(lambda x: x[1], sorted_scores))\n","    prediction_detail['runtime'] = perf_counter() - start_time\n","    prediction_detail['answer'] = data['expected_lex']\n","    prediction_detail['test_id'] = data['test_id']\n","    return json.dumps(prediction_detail)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Adgj-AhLd9j"},"outputs":[],"source":["def predict_param(data, candidates, train_len, java_tokenizer, java_model):\n","    start_time = perf_counter()\n","\n","    java_origin_context = java_tokenize_take_last(data['lex_context'],\n","                                                  tokenizer=java_tokenizer,\n","                                                  train_len=train_len)\n","    java_suggestions_all = java_tokenize_sentences(candidates, tokenizer=java_tokenizer)\n","    x_test_all = []\n","    y_test_all = []\n","    sentence_len_all = []\n","    x_test, y_test, sentence_len = prepare_sentences(java_origin_context,\n","                                                      java_suggestions_all, train_len,\n","                                                      len(java_origin_context))\n","    x_test_all += x_test.tolist()\n","    y_test_all += y_test.tolist()\n","    sentence_len_all += sentence_len\n","    x_test_all = np.array(x_test_all)\n","    y_test_all = np.array(y_test_all)\n","    p_pred = predict(java_model, x_test_all)\n","    log_p_sentence = evaluate(p_pred, y_test_all, sentence_len_all)\n","    assert len(log_p_sentence) == len(candidates)\n","    counter = 0\n","    java_suggestion_scores = []\n","    for i, java_suggestion in enumerate(java_suggestions_all):\n","        java_suggestion_scores.append((log_p_sentence[i], candidates[i]))\n","    sorted_scores = sorted(java_suggestion_scores, key=lambda x: -x[0])[:top_k]\n","    return select_top_candidates(sorted_scores, data, start_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IT4_oZVsXtbn"},"outputs":[],"source":["from pathlib import Path\n","\n","model_path = f'/content/drive/MyDrive/shared/LSTM-Kien/model/{project}/{project}.h5'\n","java_model = load_model(model_path)\n","\n","tokenizer_path = f'/content/drive/MyDrive/shared/LSTM-Kien/tokenizer/{project}/{project}.tk'\n","java_tokenizer = load(open(tokenizer_path, 'rb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1yhNye8PgsqVEQ6uVSvDq7PNeC7I34I7y"},"id":"TCpcjNYrBwxO","outputId":"9222c8ba-3d44-4283-aedb-e20421ff1c0e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["test_path = f'/content/drive/MyDrive/shared/LSTM-Kien/testcase/{project}/{project}_ArgRecTests_fold{fold}.txt'\n","tests = open(test_path, \"r\")\n","local_preds_path = f'/content/drive/MyDrive/shared/LSTM-Kien/local-pred/{project}/fold{fold}/{project}_prediction_detail_flute_sequence.txt'\n","local_preds_tests = open(local_preds_path, \"r\")\n","global_preds_path = f'/content/drive/MyDrive/shared/LSTM-Kien/global-pred/{project}/fold{fold}/{project}_prediction_detail_flute_sequence.txt'\n","Path(global_preds_path).parent.mkdir(parents=True, exist_ok=True)\n","global_preds_tests = open(global_preds_path, \"w\")\n","\n","\n","train_len = 6\n","cnt = 0\n","while True:\n","    cnt += 1\n","    # print(cnt)\n","    test = tests.readline()\n","    if test == '' or test == None:\n","        break\n","    test = json.loads(test)\n","    local_preds = json.loads(local_preds_tests.readline())\n","    if test['expected_lex'] == ')':\n","        prediction_detail = {}\n","        prediction_detail['lexModelScores'] = [0]\n","        prediction_detail['predictions'] = [')']\n","        prediction_detail['runtime'] = 0\n","        prediction_detail['answer'] = ')'\n","        prediction_detail['test_id'] = test['test_id']\n","        global_preds_tests.write(json.dumps(prediction_detail) + '\\n')\n","        continue\n","    pred_detail = predict_param(test, local_preds['predictions'][:top_k], train_len, java_tokenizer, java_model)\n","    global_preds_tests.write(pred_detail + '\\n')\n","    print(pred_detail)\n","    #print(test['test_id'], pred_detail)\n","\n","tests.close()\n","local_preds_tests.close()\n","global_preds_tests.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"r_ua1mULdju8"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"LSTM_test.ipynb","provenance":[],"mount_file_id":"1j9zwF0pz8oYfSAdSGXu_LyjKWLMnMbvX","authorship_tag":"ABX9TyNcg65ALYFiVQHprmOI2tPh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}