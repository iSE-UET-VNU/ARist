{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq4iC6vUAHc",
        "outputId": "db91504b-0885-45ba-ef18-151cc9650ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "outputs": [],
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "outputs": [],
      "source": [
        "main_path = \"drive/MyDrive/shared/GPT/\"\n",
        "repo_dir = \"eclipse_netbeans/\"\n",
        "project_name = \"eclipse\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir\n",
        "setting = \"dynamic\"\n",
        "model = \"flute\"\n",
        "filter_threshold = 20 #Default is None\n",
        "if filter_threshold is None:\n",
        "    pred_path = f\"{main_path}predictions/{repo_dir}flute_{setting}/\"\n",
        "else:\n",
        "    pred_path = f\"{main_path}predictions/{repo_dir}flute_{setting}_top_{filter_threshold}/\"\n",
        "data_path = main_path + \"data/\"\n",
        "result_path = main_path + \"results/\" + repo_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKjZVhFuX-DT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName, foldId):\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests_fold{foldId}.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syDJAophYIkG"
      },
      "outputs": [],
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    test['methodInvocClassQualifiedName'] = this['methodInvocClassQualifiedName']\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpFtWc-GYL0u"
      },
      "outputs": [],
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def readPredictions(projectName, foldId):\n",
        "    predictions = []\n",
        "    if os.path.isfile(f\"{pred_path}{projectName}_ArgRecs_fold{foldId}.txt\"):\n",
        "        filePath = f\"{pred_path}{projectName}_ArgRecs_fold{foldId}.txt\"\n",
        "\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07PbCXTYGtu8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def readFilterPreds(projectName, foldId):\n",
        "    predictions = []\n",
        "    filePath = f\"{main_path}predictions/{repo_dir}filter_{setting}/{projectName}/fold{foldId}/{projectName}_prediction_detail_flute_sequence.txt\"\n",
        "\n",
        "    if not os.path.isfile(filePath):\n",
        "        return None\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUlx4pvX4GKU"
      },
      "outputs": [],
      "source": [
        "def correctPredsOrder(preds):\n",
        "    correctPreds = []\n",
        "    for i in range(len(preds)):\n",
        "        prediction = preds[i]\n",
        "        if prediction['answer'] != ')':\n",
        "            correctPreds.append(prediction)\n",
        "    return correctPreds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def_recentness = creating_distance"
      ],
      "metadata": {
        "id": "8H1tEn_jblyw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ausAjjeCClV9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(f'{data_path}targets_def_recentness.npy', 'rb') as f:\n",
        "    targets_def_recentness = np.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqtf23bXCv_J",
        "outputId": "9f11aeba-bfe7-4427-d811-5b39f1f00060"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 0.5726471754499647,\n",
              "         1: 0.2769640318384479,\n",
              "         2: 0.0968161552075979,\n",
              "         3: 0.03216727758058745,\n",
              "         4: 0.013283750979235374,\n",
              "         5: 0.004726926313142548,\n",
              "         6: 0.0017916380552627253,\n",
              "         7: 0.0010445177326228034,\n",
              "         8: 0.0002587115680338888,\n",
              "         9: 5.561089780167703e-05,\n",
              "         10: 0.0002103542655976479,\n",
              "         11: 4.8357302436240895e-06,\n",
              "         12: 2.1760786096308403e-05,\n",
              "         13: 4.8357302436240895e-06,\n",
              "         15: 2.4178651218120447e-06})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def_recent_dict = Counter(targets_def_recentness.tolist())\n",
        "\n",
        "for key in def_recent_dict:\n",
        "    def_recent_dict[key] = def_recent_dict[key] / len(targets_def_recentness)\n",
        "\n",
        "def_recent_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use_recentness = accessing_recentness"
      ],
      "metadata": {
        "id": "Yv0l-CqcboTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(f'{data_path}targets_use_recentness.npy', 'rb') as f:\n",
        "    targets_use_recentness = np.load(f)"
      ],
      "metadata": {
        "id": "B7jTnItebqfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "use_recent_dict = Counter(targets_use_recentness)\n",
        "\n",
        "for key in use_recent_dict:\n",
        "    use_recent_dict[key] = use_recent_dict[key] / len(targets_use_recentness)\n",
        "\n",
        "use_recent_dict"
      ],
      "metadata": {
        "id": "okcJLLhgbqz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "outputs": [],
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7mZtwMPHBOr"
      },
      "outputs": [],
      "source": [
        "def preprocess_filter(candidate):\n",
        "    candidate = preprocessor.empty_string_literal(candidate)\n",
        "    if \"{\" in candidate:\n",
        "        candidate = candidate[:candidate.index(\"{\")].rstrip()\n",
        "    if \"]\" in candidate:\n",
        "        candidate = preprocessor.remove_array_access_index(candidate)\n",
        "    if \"(\" in candidate and candidate.index(\"(\") > 0:\n",
        "        candidate = preprocessor.normalize_method_invocation(candidate)\n",
        "\n",
        "    # Lambda expression\n",
        "    if \"->\" in candidate:\n",
        "        candidate = \"x -> {}\"\n",
        "\n",
        "    # Exclude candidates starting with this if they are redundant\n",
        "    if candidate.startswith(\"this.\"):\n",
        "        candidate = candidate[5:]\n",
        "\n",
        "    return candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dNJtvsCHB1S"
      },
      "outputs": [],
      "source": [
        "def preprocess_all_filter_preds(filters_predictions):  \n",
        "  for i in range(len(filters_predictions)):\n",
        "      for j in range(len(filters_predictions[i]['predictions'])):\n",
        "          filters_predictions[i]['predictions'][j] = preprocess_filter(filters_predictions[i]['predictions'][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN-nKkwg6Va8"
      },
      "outputs": [],
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and result == \"<LAMBDA>\":\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5YpySsDlz_m"
      },
      "outputs": [],
      "source": [
        "def canAcceptResult(test, result):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    result = preprocess(result)\n",
        "    if '{' in result:\n",
        "        result = result[:result.index('{')].rstrip()\n",
        "    if result.find('(') > 0:\n",
        "        result = preprocessor.normalize_method_invocation(result)\n",
        "    \n",
        "    if matchesArg(expectedLex, result):\n",
        "        return True\n",
        "\n",
        "    alternateLex = None\n",
        "    if 'methodAccessLex' in test:\n",
        "        alternateLex = test['methodAccessLex']\n",
        "    if 'objectCreationLex' in test:\n",
        "        alternateLex = test['objectCreationLex']\n",
        "    if alternateLex is not None and matchesArg(alternateLex, result):\n",
        "        return True\n",
        "\n",
        "    if 'staticMemberAccessLex' in test:\n",
        "        if matchesArg(test['staticMemberAccessLex'], result):\n",
        "            return True\n",
        "            \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUqA-mJ36SFm"
      },
      "outputs": [],
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L82NnK3naslK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "os.makedirs('results/' + project_name, exist_ok=True)\n",
        "os.makedirs('logs/' + project_name, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b96CurBBe5ET"
      },
      "outputs": [],
      "source": [
        "dataFrame = defaultdict(list)\n",
        "\n",
        "def updateTopKResult(test, results, k, adequateGeneratedCandidate, doPrintIncorrectPrediction, projectName):\n",
        "    isOverallCorrectTopK = False\n",
        "    for i in range(min(k, len(results))):\n",
        "        if canAcceptResult(test, results[i]):\n",
        "            isOverallCorrectTopK = True\n",
        "            break\n",
        "\n",
        "    argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else 'null'\n",
        "    if isOverallCorrectTopK:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(1)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(1)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(1)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(1)\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(1)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(1)\n",
        "    else:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(0)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(0)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(0)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(0)\n",
        "            if doPrintIncorrectPrediction:\n",
        "                outputFileName = f\"{projectName}_incorrect_ArgRecTests_top_{k}.txt\"\n",
        "                with open(f'logs/{repo_dir}{outputFileName}', \"a\") as f:\n",
        "                    f.write(json.dumps(test['argRecTestList'][0]) + '\\n')\n",
        "                    f.write('Predictions: ' + str(results) + '\\n')\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(0)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjGHglP0EFv0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "W_GPT = 1\n",
        "W_LOCALITY = 1\n",
        "\n",
        "def combine_score(main_score, filter_score, candidate):\n",
        "    return main_score * W_GPT + filter_score * (1 - W_GPT)\n",
        "    #return np.log(np.exp(main_score) * W_GPT + np.exp(filter_score) * (1 - W_GPT))\n",
        "    #return max(main_score, filter_score)\n",
        "    #return min(main_score, filter_score)\n",
        "    #return score_by_type(main_score, filter_score, candidate)\n",
        "\n",
        "def combine_score_all_feature(lex_score, locality_score):\n",
        "    return (lex_score + W_LOCALITY * locality_score) / (1 + W_LOCALITY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzjVqmp-3Eg"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "COMPOUND_CONSIDERED = False\n",
        "TEST_APIS = [\n",
        "            #  \"org.eclipse.swt\",\n",
        "            #  \"java.awt\",\n",
        "            #  \"javax.swing\",\n",
        "]\n",
        "if len(TEST_APIS) > 0:\n",
        "    model += \"__lib\"\n",
        "TEST_LOCAL_ARG = None\n",
        "if TEST_LOCAL_ARG is not None:\n",
        "    if TEST_LOCAL_ARG:\n",
        "        model += \"__local_arg\"\n",
        "    else:\n",
        "        model += \"__not_local_arg\"\n",
        "if filter_threshold is not None:\n",
        "    model += f\"__top_{filter_threshold}\"\n",
        "\n",
        "rank_list = []\n",
        "\n",
        "fold_list = []\n",
        "for testFile in os.listdir(pred_path):\n",
        "    if testFile.startswith(f\"{project_name}_ArgRecs_fold\"):\n",
        "        fold_id = testFile[testFile.find(\"fold\")+4:][0]\n",
        "        fold_list.append(fold_id)\n",
        "\n",
        "for fold_id in tqdm(fold_list):\n",
        "    tests = readTests(project_name, fold_id)\n",
        "    predictions = readPredictions(project_name, fold_id)\n",
        "    filters_predictions = readFilterPreds(project_name, fold_id)\n",
        "    filters_predictions = correctPredsOrder(filters_predictions)\n",
        "    tests = allTestsToSingleArgRecTest(tests)\n",
        "    preprocess_all_filter_preds(filters_predictions)\n",
        "\n",
        "    assert len(tests) == len(predictions), \"Tests not matched!\"\n",
        "    for i in range(len(tests)):\n",
        "        assert tests[i]['expected_lex'][:3] == predictions[i]['answer'][:3], \"Tests not matched!\"\n",
        "\n",
        "    assert len(tests) == len(filters_predictions), \"Tests not matched!\"\n",
        "    for i in range(len(tests)):\n",
        "        if tests[i]['expected_lex'][:3] != filters_predictions[i]['answer'][:3]:\n",
        "            print(tests[i]['expected_lex'])\n",
        "            print(filters_predictions[i]['answer'])\n",
        "        assert tests[i]['expected_lex'][:3] == filters_predictions[i]['answer'][:3], \"Tests not matched!\"\n",
        "\n",
        "    for i in range(len(tests)):\n",
        "        test = tests[i]\n",
        "        dataFrame['Tested'].append(1)\n",
        "\n",
        "        if test['numArg'] == 0:\n",
        "            continue\n",
        "        if len(TEST_APIS) > 0:\n",
        "            is_target = False\n",
        "            for target_api in TEST_APIS:\n",
        "                if test['methodInvocClassQualifiedName'].startswith(target_api + '.'):\n",
        "                    is_target = True\n",
        "            if not is_target:\n",
        "                continue\n",
        "        if TEST_LOCAL_ARG is not None:\n",
        "            if not predictions[i]['sufficient_candidates']:\n",
        "                continue\n",
        "            is_local_arg = False\n",
        "            for j in range(len(test['next_lex'][0])):\n",
        "                for k in range(len(test['next_lex'][0][j])):\n",
        "                    candidate = test['next_lex'][0][j][k]\n",
        "                    if candidate == test['expected_lex']:\n",
        "                        candidate_locality = test['argRecTestList'][0]['candidates_locality'][j][k]\n",
        "                        if candidate_locality >= 4:\n",
        "                            is_local_arg = True\n",
        "                            break\n",
        "                if is_local_arg:\n",
        "                    break\n",
        "            if TEST_LOCAL_ARG != is_local_arg:\n",
        "                continue\n",
        "\n",
        "        dataFrame['Predicted'].append(1)\n",
        "        if not test['ignored']:\n",
        "            dataFrame['Predicted supported'].append(1)\n",
        "\n",
        "        oneArgTest = test['argRecTestList'][0]\n",
        "        next_lex_locality_dict = {}\n",
        "        for j in range(len(oneArgTest['next_lex'])):\n",
        "            for k in range(len(oneArgTest['next_lex'][j])):\n",
        "                candidate = oneArgTest['next_lex'][j][k]\n",
        "                scope_distance = oneArgTest['candidates_scope_distance'][j][k]\n",
        "                lu_distance = oneArgTest['candidates_last_usage_distance'][j][k]\n",
        "                if scope_distance >= 0:\n",
        "                    next_lex_locality_dict[candidate] = def_recent_dict[scope_distance]\n",
        "                    #next_lex_locality_dict[candidate] *= use_recent_dict[lu_distance]\n",
        "        \n",
        "        response = predictions[i]\n",
        "        gptResults = response['predictions']\n",
        "        gptScores = response['scores']\n",
        "        runtime = response['runtime']\n",
        "        if not COMPOUND_CONSIDERED:\n",
        "            for k in range(len(gptResults)):\n",
        "                if gptResults[k] == '<COMPOUND>':\n",
        "                    gptResults = gptResults[:k] + gptResults[k + 1:]\n",
        "                    gptScores = gptScores[:k] + gptScores[k + 1:]\n",
        "                    break\n",
        "\n",
        "        prediction_dict = {}\n",
        "        lex_sim_candidate_dict = {}\n",
        "        for j in range(len(filters_predictions[i]['predictions']) - 1, -1, -1):\n",
        "            candidate = filters_predictions[i]['predictions'][j]\n",
        "            prediction_dict[candidate] = filters_predictions[i]['lexModelScores'][j]\n",
        "            if filters_predictions[i]['lexModelScores'][j] > 0:\n",
        "                #prediction_dict[candidate] = LOG_ZERO\n",
        "                print(f\"Bug: project {project_name} - fold {fold_id} - test id {i} - prediction id {j}\")\n",
        "\n",
        "            lex_sim = filters_predictions[i]['lexSimScores'][j]\n",
        "            # lex_sim_score = np.log(lex_sim_dict[get_bin(np.exp(lex_sim))])\n",
        "            # lex_sim_candidate_dict[candidate] = lex_sim_score\n",
        "\n",
        "        for j in range(len(gptResults)):\n",
        "            prediction_dict[gptResults[j]] = combine_score(gptScores[j], prediction_dict[gptResults[j]], gptResults[j])\n",
        "            # prediction_dict[gptResults[j]] = prediction_dict[gptResults[j]] + lex_sim_candidate_dict[gptResults[j]] * 0.5\n",
        "            # prediction_dict[gptResults[j]] = prediction_dict[gptResults[j]] + lex_sim * 0.5\n",
        "\n",
        "            if gptResults[j] in next_lex_locality_dict:\n",
        "                prediction_dict[gptResults[j]] = combine_score_all_feature(prediction_dict[gptResults[j]], np.log(next_lex_locality_dict[gptResults[j]]))\n",
        "\n",
        "        combinedResults = sorted(list(set(filters_predictions[i]['predictions'])), key=lambda x: -prediction_dict[x])\n",
        "\n",
        "        rank = -1\n",
        "        for k in range(min(10, len(combinedResults))):\n",
        "            if canAcceptResult(test, combinedResults[k]):\n",
        "                rank = k\n",
        "        rank_list.append(rank)\n",
        "\n",
        "        for k in tops:\n",
        "            updateTopKResult(test, combinedResults, k, response['sufficient_candidates'], False, project_name)\n",
        "\n",
        "        dataFrame[\"GPT's runtime\"].append(runtime)\n",
        "        argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else None\n",
        "        if argType is not None:\n",
        "            dataFrame[\"ArgType\"].append(expressionTypeDict[argType])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf1Uzu_hBotA",
        "outputId": "2836ca58-9ca2-4c65-c9e5-d50fdef5c267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7125629791584323\n"
          ]
        }
      ],
      "source": [
        "MRR = 0\n",
        "for rank in rank_list:\n",
        "    if rank < 0:\n",
        "        MRR += 0\n",
        "    else:\n",
        "        MRR += 1/(rank + 1)\n",
        "MRR /= len(rank_list)\n",
        "print(MRR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtmIUfQaNrr-"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('logs/dataframe.pkl', 'wb') as f:\n",
        "#     pickle.dump(dataFrame, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXCB5cWdvYKp",
        "outputId": "a9ef2412-8d04-4f5e-b6c6-b3a7022637d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def printTestResult():\n",
        "    with open(f'results/{project_name}/arg_rec_{model}_log.txt', 'w') as f:\n",
        "        f.write(f\"Ran {len(dataFrame['Tested'])} tests successfully.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted'])} tests.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted supported'])} tests that were supported.\\n\")\n",
        "        f.write(f\"Skipped {len(dataFrame['Tested']) - len(dataFrame['Predicted'])} tests. They were not taken into account during evaluation.\\n\")\n",
        "        gptRuntime = np.mean(dataFrame[\"GPT's runtime\"])\n",
        "        f.write(f\"GPT's runtime: {gptRuntime}s\\n\")\n",
        "        f.write(f\"MRR: {MRR}\\n\")\n",
        "\n",
        "    accuracyPerNumArg = []\n",
        "    row = []\n",
        "    row.append(\"Number of params\")\n",
        "    row.append(\"Percentage of distribution\")\n",
        "    for k in tops:\n",
        "        row.append(f\"GPT's top-{k} accuracy\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} precision\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} recall\")\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    unique, counts = np.unique(dataFrame['ArgType'], return_counts=True)\n",
        "    counts = counts / counts.sum()\n",
        "    argTypeDict = defaultdict(float)\n",
        "    for i in range(len(unique)):\n",
        "        argTypeDict[unique[i]] = counts[i]\n",
        "\n",
        "    for i in range(len(expressionTypes)):\n",
        "        argType = expressionTypes[i]\n",
        "        row = []\n",
        "        row.append(argType)\n",
        "        row.append(argTypeDict[i] * 100)\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTOverallTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTActualTop{k}{argType}\"]))\n",
        "        accuracyPerNumArg.append(row)\n",
        "\n",
        "    row = []\n",
        "    row.append(\"all\")\n",
        "    row.append(\"100\")\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTOverallTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTActualTop{k}\"]))\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    with open(f'results/{project_name}/arg_rec_{model}.csv', 'w') as f:\n",
        "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        for row in accuracyPerNumArg:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    with open(f'results/{project_name}/arg_rec_{model}_log.txt', 'a') as f:\n",
        "        for k in tops:\n",
        "            correctTestsCount = np.sum(dataFrame[f\"GPTActualTop{k}\"])\n",
        "            f.write(f\"Target showed up in top {k} recommendations in {correctTestsCount} tests.\\n\")\n",
        "\n",
        "printTestResult()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4neBPx5Q4ecd"
      },
      "outputs": [],
      "source": [
        "for text_file in os.listdir('results/' + project_name):\n",
        "    shutil.copyfile('results/' + project_name + '/' + text_file, main_path + 'results/' + project_name + '/' + text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb-Qex8wPUXg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ARist_evaluate_small_corpus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}