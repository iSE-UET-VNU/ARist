{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKYC1d_h-1T7"
      },
      "source": [
        "## CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvgme1E3IJfB",
        "outputId": "8ba53af3-b9f4-4e27-c489-c15c768075b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-dev libnvinfer-plugin7 libnvinfer7\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 143 MB of archives.\n",
            "After this operation, 520 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer7 7.0.0-1+cuda10.0 [69.6 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-dev 7.0.0-1+cuda10.0 [71.6 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer-plugin7 7.0.0-1+cuda10.0 [2,109 kB]\n",
            "Fetched 143 MB in 3s (53.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer7.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvinfer7_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer7 (7.0.0-1+cuda10.0) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../libnvinfer-dev_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (7.0.0-1+cuda10.0) ...\n",
            "Selecting previously unselected package libnvinfer-plugin7.\n",
            "Preparing to unpack .../libnvinfer-plugin7_7.0.0-1+cuda10.0_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin7 (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer7 (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer-dev (7.0.0-1+cuda10.0) ...\n",
            "Setting up libnvinfer-plugin7 (7.0.0-1+cuda10.0) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y --no-install-recommends libnvinfer7=7.0.0-1+cuda10.0 \\\n",
        "    libnvinfer-dev=7.0.0-1+cuda10.0 \\\n",
        "    libnvinfer-plugin7=7.0.0-1+cuda10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Installing necessary libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "117c4c80-05f7-4d0b-e570-b1265c0eefd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.37.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.42.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.13.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=1a3deeedaf7a6b37740639e910abd4c4bbe33aeb95a91e2615518e16902590a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow-gpu\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 tensorflow-gpu-1.15.2\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "#!pip install tensorflow==1.15.2\n",
        "!pip install tensorflow-gpu==1.15.2\n",
        "!pip install -q gpt-2-simple==0.7.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQFo2PINvblo",
        "outputId": "5f48528e-ca40-4f32-b917-053e9ec7da9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## Verifying GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmTooTW3osf",
        "outputId": "0c3afcd4-731d-4df5-bdbc-000f4cd2f3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 23 20:43:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgvl-BJ7I4H",
        "outputId": "b5224449-5c24-407b-962c-0520be247c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11559251388955632759\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 1260313731705644774\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 922248498910722416\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15964005991\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7960961174839188233\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxKEhGLhQ2H0",
        "outputId": "74aedad9-31c8-49b5-ee4c-56af5cf61744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "outputs": [],
      "source": [
        "main_path = \"drive/MyDrive/shared/GPT/\"\n",
        "repo_dir = \"eclipse_netbeans/\"\n",
        "project_name = \"eclipse\"\n",
        "fold_id = 0\n",
        "setting = \"dynamic\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name=f'fold{fold_id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generating candidates using GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "outputs": [],
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlD93fzPsmo_"
      },
      "outputs": [],
      "source": [
        "from gpt.gpt_manager_full import GPTManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVeJYgBAXT3h"
      },
      "outputs": [],
      "source": [
        "gpt_manager = GPTManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cb7co1DZthy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94bd30c-9f5c-4c3c-a36e-1cb3ca465a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "244 end tokens.\n",
            "Loading model checkpoint/run1/model-50000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-50000\n",
            "WARNING:tensorflow:From /content/gpt/sample.py:33: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ],
      "source": [
        "gpt_manager.java_model = gpt_manager.load_model('checkpoint/', 'run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName, foldId):\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests_fold{foldId}.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D8DCdMR7o2f"
      },
      "outputs": [],
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWsJFtYc-Ml_"
      },
      "outputs": [],
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "outputs": [],
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6HyRqvGa6fY"
      },
      "outputs": [],
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaWz9QQsAXoV"
      },
      "outputs": [],
      "source": [
        "def canAcceptGeneratedLexes(test):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    for group in test['next_lex']:\n",
        "        for candidate in group:\n",
        "            candidate = preprocess(candidate)\n",
        "            if matchesArg(expectedLex, candidate):\n",
        "                return True\n",
        "\n",
        "            alternateLex = None\n",
        "            if 'methodAccessLex' in test:\n",
        "                alternateLex = test['methodAccessLex']\n",
        "            if 'objectCreationLex' in test:\n",
        "                alternateLex = test['objectCreationLex']\n",
        "            if alternateLex is not None and matchesArg(alternateLex, candidate):\n",
        "                return True\n",
        "\n",
        "            if 'staticMemberAccessLex' in test:\n",
        "                if matchesArg(test['staticMemberAccessLex'], candidate):\n",
        "                    return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUdqQfcBbA0A"
      },
      "outputs": [],
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqn5PBv8c19j"
      },
      "outputs": [],
      "source": [
        "def validateTest(test):\n",
        "    if not test['ignored']:\n",
        "        adequateGeneratedLex = False\n",
        "        if canAcceptGeneratedLexes(test):\n",
        "            adequateGeneratedLex = True;\n",
        "        if adequateGeneratedLex:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzjVqmp-3Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7609467f-752f-4705-8379-02a6b3e6b5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15817\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "os.makedirs(f'predictions/{repo_dir}gpt2/', exist_ok=True)\n",
        "\n",
        "predictionFilePath = \"predictions/\" + repo_dir + \"gpt2/\" + f\"{project_name}_ArgRecs_fold{fold_id}.txt\"\n",
        "prediction_details = []\n",
        "tests = allTestsToSingleArgRecTest(readTests(project_name, fold_id))\n",
        "cnt = -1\n",
        "\n",
        "if not os.path.isfile(main_path + predictionFilePath):\n",
        "    startId = 0\n",
        "else:\n",
        "    with open(main_path + predictionFilePath) as f:\n",
        "        content = f.read()\n",
        "        startId = content.count('\\n')\n",
        "        print(startId)\n",
        "        lines = content.split('\\n')\n",
        "        totalRuntime = 0\n",
        "        for line in lines[:-1]:\n",
        "            prediction = json.loads(line)\n",
        "            totalRuntime += prediction['runtime']\n",
        "\n",
        "filePath = f\"{main_path}predictions/{repo_dir}/filter_{setting}/{project_name}/fold{fold_id}/{project_name}_prediction_detail_flute_sequence.txt\"\n",
        "filter_preds = []\n",
        "with open(filePath) as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for line in lines[:-1]:\n",
        "        filter_pred = json.loads(line)\n",
        "        if filter_pred['answer'] != ')':\n",
        "            filter_preds.append(filter_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(startId, len(tests))):\n",
        "    test = tests[i]\n",
        "    if test['numArg'] == 0:\n",
        "        continue\n",
        "\n",
        "    response = gpt_manager.generate_param(test)\n",
        "    gptResults = response['result']\n",
        "\n",
        "    prediction_detail = {}\n",
        "    prediction_detail['test_id'] = test['id']\n",
        "    prediction_detail['predictions'] = gptResults\n",
        "    prediction_detail['answer'] = test['expected_lex']\n",
        "    prediction_detail['arg_type'] = test['argRecTestList'][0]['argType']\n",
        "    prediction_detail['runtime'] = response['runtime']\n",
        "    prediction_detail['ignored'] = test['ignored']\n",
        "    prediction_detail['sufficient_candidates'] = validateTest(test)\n",
        "    #prediction_details.append(prediction_detail)\n",
        "\n",
        "    with open(main_path + predictionFilePath, \"a\") as f:\n",
        "        f.write(json.dumps(prediction_detail) + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0e071b45f5ba4f4cb286af2ffdfa3cb2",
            "db858556afb048039c98495b5649677e",
            "b5dd5019b9d849d4b3ba19160a9b0bd4",
            "22fa55f8c7224918995a028155a6a622",
            "2adb3590d32b4ddabb68c3dc8a59f1bb",
            "e5ee6555b1da43ac85de0bc9b33b7419",
            "a375aa7db12746809dc5cb20ae174a09",
            "bda3f24b6bc8426aa631d2173431604c",
            "c6954a15d16b4ed580631541bdec2098",
            "9b1f18b52dbc4133ab075bf021e1219d",
            "ce4c63021e814033a565499e09a8f8ad"
          ]
        },
        "id": "gzhJ3xNvySBc",
        "outputId": "ab59816d-f9a4-4476-c37d-5bd5795ed3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e071b45f5ba4f4cb286af2ffdfa3cb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15047 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/gpt/gpt_manager_full.py:580: RuntimeWarning: invalid value encountered in log\n",
            "  log_prob = np.log(prob)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(predictionFilePath, \"w\") as f:\n",
        "#     for prediction_detail in prediction_details:\n",
        "#         f.write(json.dumps(prediction_detail) + '\\n')\n",
        "\n",
        "# shutil.copyfile(predictionFilePath, main_path + predictionFilePath)"
      ],
      "metadata": {
        "id": "U1r9g_5BRXZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "outputs": [],
      "source": [
        "!kill -9 -1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xbulk8mEHdzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "GPT2_predict_small_corpus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e071b45f5ba4f4cb286af2ffdfa3cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db858556afb048039c98495b5649677e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b5dd5019b9d849d4b3ba19160a9b0bd4",
              "IPY_MODEL_22fa55f8c7224918995a028155a6a622",
              "IPY_MODEL_2adb3590d32b4ddabb68c3dc8a59f1bb"
            ]
          }
        },
        "db858556afb048039c98495b5649677e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5dd5019b9d849d4b3ba19160a9b0bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5ee6555b1da43ac85de0bc9b33b7419",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a375aa7db12746809dc5cb20ae174a09"
          }
        },
        "22fa55f8c7224918995a028155a6a622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bda3f24b6bc8426aa631d2173431604c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15047,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15047,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6954a15d16b4ed580631541bdec2098"
          }
        },
        "2adb3590d32b4ddabb68c3dc8a59f1bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b1f18b52dbc4133ab075bf021e1219d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15047/15047 [5:03:32&lt;00:00,  2.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce4c63021e814033a565499e09a8f8ad"
          }
        },
        "e5ee6555b1da43ac85de0bc9b33b7419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a375aa7db12746809dc5cb20ae174a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bda3f24b6bc8426aa631d2173431604c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6954a15d16b4ed580631541bdec2098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b1f18b52dbc4133ab075bf021e1219d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce4c63021e814033a565499e09a8f8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}