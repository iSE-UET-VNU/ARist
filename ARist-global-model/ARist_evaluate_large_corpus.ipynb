{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARist_evaluate_large_corpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcdb86d189814622a0ea9e3502def112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_390f5ad0bd3c4a658290dc45915a2b5a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afc8720cf4974762aecff9b0650a6e7a",
              "IPY_MODEL_53e256cdf8784bcab66e6bcd6fefc406",
              "IPY_MODEL_79eabc1b42054c9fbe4c6e18b6039a2b"
            ]
          }
        },
        "390f5ad0bd3c4a658290dc45915a2b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afc8720cf4974762aecff9b0650a6e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f11b1ba806954e5185c0c64f5f5f501b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c1d4759b8184917bca617243de07bc9"
          }
        },
        "53e256cdf8784bcab66e6bcd6fefc406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ddad900044d741d0bbf5c21b657a23fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 387,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 387,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_684238a056b64b7890207928bd0c08d0"
          }
        },
        "79eabc1b42054c9fbe4c6e18b6039a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_803025e426544e409dc02cff30e02949",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 387/387 [16:53&lt;00:00, 10.66s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19e3950c982849feb14e87ab8f27e13d"
          }
        },
        "f11b1ba806954e5185c0c64f5f5f501b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c1d4759b8184917bca617243de07bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddad900044d741d0bbf5c21b657a23fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "684238a056b64b7890207928bd0c08d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "803025e426544e409dc02cff30e02949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19e3950c982849feb14e87ab8f27e13d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puq4iC6vUAHc",
        "outputId": "9fe8298c-6af4-4dd5-9538-f6c9c9dc60f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_TNaVORx5Mb"
      },
      "source": [
        "!cp -R drive/MyDrive/shared/GPT/gpt gpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREDICT_PERCENT = 100"
      ],
      "metadata": {
        "id": "lmFcuSyHY61N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9iwWx-Lc4oR"
      },
      "source": [
        "main_path = \"drive/MyDrive/shared/GPT/\"\n",
        "repo_dir = \"four_hundred/\"\n",
        "tests_path = main_path + \"tests/\" + repo_dir\n",
        "setting = \"dynamic\"\n",
        "if setting == \"maintenance\":\n",
        "    PREDICT_PERCENT = 100\n",
        "model = f\"flute_{setting}\"\n",
        "filter_threshold = 20 #Default is None\n",
        "if filter_threshold is None:\n",
        "    pred_path = f\"{main_path}predictions/{repo_dir}flute_{setting}/\"\n",
        "else:\n",
        "    pred_path = f\"{main_path}predictions/{repo_dir}flute_{setting}_top_{filter_threshold}/\"\n",
        "data_path = main_path + \"data/\"\n",
        "result_path = main_path + \"results/\" + repo_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def readTests(projectName):\n",
        "    if setting == \"maintenance\":\n",
        "        filePathSet = set()\n",
        "        with open(f\"{main_path}test_file_paths/maintenance_files.txt\") as f:\n",
        "            lines = f.read().split('\\n')\n",
        "            for line in lines:\n",
        "                if len(line) > 0:\n",
        "                  filePathSet.add(line[line.index(\"java_repos/\")+len(\"java_repos/\"):])\n",
        "\n",
        "    oneArgTests = []\n",
        "    with open(f\"{tests_path}{projectName}_ArgRecTests.txt\") as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTest = json.loads(line)\n",
        "            if setting != \"maintenance\" or (projectName + '/' + oneArgTest['filePath']) in filePathSet:\n",
        "                oneArgTests.append(oneArgTest)\n",
        "        lines = None\n",
        "    return oneArgTests"
      ],
      "metadata": {
        "id": "jKjZVhFuX-DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def toSingleArgRecTest(this):\n",
        "    test = {}\n",
        "    test['filePath'] = this['filePath']\n",
        "    test['line'] = this['line']\n",
        "    test['col'] = this['col']\n",
        "    test['numArg'] = 1 if this['argPos'] != 0 else 0\n",
        "    test['lex_context'] = this['lex_context']\n",
        "    test['excode_context'] = this['excode_context']\n",
        "    test['next_excode'] = [this['next_excode']]\n",
        "    test['next_lex'] = [this['next_lex']]\n",
        "    test['expected_excode'] = this['expected_excode']\n",
        "    test['expected_lex'] = this['expected_lex']\n",
        "    test['ignored'] = this['ignored']\n",
        "    test['argRecTestList'] = [this]\n",
        "    test['id'] = this['test_id']\n",
        "    return test"
      ],
      "metadata": {
        "id": "syDJAophYIkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def allTestsToSingleArgRecTest(oneArgTests):\n",
        "    tests = []\n",
        "    for i in range(len(oneArgTests)):\n",
        "        test = oneArgTests[i]\n",
        "        # SKIP METHOD INVOCATIONS WITH NO ARGUMENT PASSED\n",
        "        if test['argPos'] > 0:\n",
        "            test = toSingleArgRecTest(test)\n",
        "            tests.append(test)\n",
        "    return tests"
      ],
      "metadata": {
        "id": "PpFtWc-GYL0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EgXSI_Gdimb"
      },
      "source": [
        "import os\n",
        "\n",
        "def readPredictions(projectName):\n",
        "    predictions = []\n",
        "    if os.path.isfile(f\"{pred_path}{projectName}_ArgRecs.txt\"):\n",
        "        filePath = f\"{pred_path}{projectName}_ArgRecs.txt\"\n",
        "    else:\n",
        "        if filter_threshold is None:\n",
        "            filePath = main_path + f\"predictions_{PREDICT_PERCENT}/{repo_dir}flute_{setting}/{projectName}_ArgRecs.txt\"\n",
        "        else:\n",
        "            filePath = main_path + f\"predictions_{PREDICT_PERCENT}/{repo_dir}flute_{setting}_top_{filter_threshold}/{projectName}_ArgRecs.txt\"\n",
        "\n",
        "    if not os.path.isfile(filePath):\n",
        "        return None\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def readFilterPreds(projectName):\n",
        "    predictions = []\n",
        "    filePath = f\"{main_path}predictions/{repo_dir}filter_{setting}/{projectName}/{projectName}_prediction_detail_flute_sequence.txt\"\n",
        "\n",
        "    if not os.path.isfile(filePath):\n",
        "        return None\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            predictions.append(json.loads(line))\n",
        "        lines = None\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "07PbCXTYGtu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def readMapping(projectName):\n",
        "    with open(f\"{main_path + 'test_mappings/'}{projectName}_ArgRecTests_mapping.npy\", 'rb') as f:\n",
        "        return np.load(f)"
      ],
      "metadata": {
        "id": "8aYubEWTGza9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correctPredsOrder(preds, test_mapping):\n",
        "    correctPreds = []\n",
        "    if setting != \"maintenance\":\n",
        "        for i in range(len(test_mapping)):\n",
        "            prediction = preds[test_mapping[i]]\n",
        "            if prediction['answer'] != ')':\n",
        "                correctPreds.append(prediction)\n",
        "    else:\n",
        "        for i in range(len(preds)):\n",
        "            prediction = preds[i]\n",
        "            if prediction['answer'] != ')':\n",
        "                correctPreds.append(prediction)\n",
        "    return correctPreds"
      ],
      "metadata": {
        "id": "k6Wd6xDBG9Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readAppendices(projectName):\n",
        "    filePath = f\"{main_path}appendices/{repo_dir}{projectName}_ArgRecTestAppendices.txt\"\n",
        "    if not os.path.isfile(filePath):\n",
        "        return None\n",
        "    oneArgTests = []\n",
        "    with open(filePath) as f:\n",
        "        lines = f.read().split('\\n')\n",
        "        for line in lines[:-1]:\n",
        "            oneArgTests.append(json.loads(line))\n",
        "        lines = None\n",
        "    return oneArgTests"
      ],
      "metadata": {
        "id": "WxlF76ePCJnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def_recentness = creating_distance"
      ],
      "metadata": {
        "id": "2Ji_uvuYTLfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(f'{data_path}targets_def_recentness.npy', 'rb') as f:\n",
        "    targets_def_recentness = np.load(f)"
      ],
      "metadata": {
        "id": "ausAjjeCClV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def_recent_dict = Counter(targets_def_recentness)\n",
        "\n",
        "for key in def_recent_dict:\n",
        "    def_recent_dict[key] = def_recent_dict[key] / len(targets_def_recentness)\n",
        "\n",
        "def_recent_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqtf23bXCv_J",
        "outputId": "f3c7ff84-9da3-4c22-8d72-53c947b01eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 0.5726471754499647,\n",
              "         1: 0.2769640318384479,\n",
              "         2: 0.0968161552075979,\n",
              "         3: 0.03216727758058745,\n",
              "         4: 0.013283750979235374,\n",
              "         5: 0.004726926313142548,\n",
              "         6: 0.0017916380552627253,\n",
              "         7: 0.0010445177326228034,\n",
              "         8: 0.0002587115680338888,\n",
              "         9: 5.561089780167703e-05,\n",
              "         10: 0.0002103542655976479,\n",
              "         11: 4.8357302436240895e-06,\n",
              "         12: 2.1760786096308403e-05,\n",
              "         13: 4.8357302436240895e-06,\n",
              "         15: 2.4178651218120447e-06})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use_recentness = accessing_recentness"
      ],
      "metadata": {
        "id": "k8JVxciYzRPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(f'{data_path}targets_use_recentness.npy', 'rb') as f:\n",
        "    targets_use_recentness = np.load(f)"
      ],
      "metadata": {
        "id": "x8J5etjozcTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "use_recent_dict = Counter(targets_use_recentness)\n",
        "\n",
        "for key in use_recent_dict:\n",
        "    use_recent_dict[key] = use_recent_dict[key] / len(targets_use_recentness)\n",
        "\n",
        "use_recent_dict"
      ],
      "metadata": {
        "id": "lxCiJhKQzc9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MGnjcGWy6dH"
      },
      "source": [
        "from gpt import preprocessor\n",
        "\n",
        "def preprocess(target):\n",
        "    target = preprocessor.empty_string_literal(target)\n",
        "    target = preprocessor.remove_array_access_index(target)\n",
        "    return target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_filter(candidate):\n",
        "    candidate = preprocessor.empty_string_literal(candidate)\n",
        "    if \"{\" in candidate:\n",
        "        candidate = candidate[:candidate.index(\"{\")].rstrip()\n",
        "    if \"]\" in candidate:\n",
        "        candidate = preprocessor.remove_array_access_index(candidate)\n",
        "    if \"(\" in candidate and candidate.index(\"(\") > 0:\n",
        "        candidate = preprocessor.normalize_method_invocation(candidate)\n",
        "\n",
        "    # Lambda expression\n",
        "    if \"->\" in candidate:\n",
        "        candidate = \"x -> {}\"\n",
        "\n",
        "    # Exclude candidates starting with this if they are redundant\n",
        "    if candidate.startswith(\"this.\"):\n",
        "        candidate = candidate[5:]\n",
        "\n",
        "    return candidate"
      ],
      "metadata": {
        "id": "J7mZtwMPHBOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_all_filter_preds(filters_predictions):  \n",
        "  for i in range(len(filters_predictions)):\n",
        "      for j in range(len(filters_predictions[i]['predictions'])):\n",
        "          filters_predictions[i]['predictions'][j] = preprocess_filter(filters_predictions[i]['predictions'][j])"
      ],
      "metadata": {
        "id": "9dNJtvsCHB1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN-nKkwg6Va8"
      },
      "source": [
        "def matchesArg(expectedLex, result):\n",
        "    if result == expectedLex:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and '->' in result:\n",
        "        return True\n",
        "\n",
        "    if '->' in expectedLex and result == \"<LAMBDA>\":\n",
        "        return True\n",
        "\n",
        "    if '.this' in expectedLex:\n",
        "        if matchesArg(expectedLex[expectedLex.index('.this')+1:], result):\n",
        "            return True\n",
        "\n",
        "    if '.this' in result:\n",
        "        if matchesArg(expectedLex, result[result.index('.this')+1:]):\n",
        "            return True\n",
        "\n",
        "    if expectedLex.startswith('this.'):\n",
        "        if matchesArg(expectedLex[5:], result):\n",
        "            return True\n",
        "\n",
        "    if result.startswith('this.'):\n",
        "        if matchesArg(expectedLex, result[5:]):\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YpySsDlz_m"
      },
      "source": [
        "def canAcceptResult(test, result):\n",
        "    test = test['argRecTestList'][0]\n",
        "\n",
        "    expectedLex = test['expected_lex']\n",
        "\n",
        "    expectedLex = preprocess(expectedLex)\n",
        "    if '{' in expectedLex:\n",
        "        expectedLex = expectedLex[:expectedLex.index('{')].rstrip()\n",
        "\n",
        "    result = preprocess(result)\n",
        "    if '{' in result:\n",
        "        result = result[:result.index('{')].rstrip()\n",
        "    if result.find('(') > 0:\n",
        "        result = preprocessor.normalize_method_invocation(result)\n",
        "    \n",
        "    if matchesArg(expectedLex, result):\n",
        "        return True\n",
        "\n",
        "    alternateLex = None\n",
        "    if 'methodAccessLex' in test:\n",
        "        alternateLex = test['methodAccessLex']\n",
        "    if 'objectCreationLex' in test:\n",
        "        alternateLex = test['objectCreationLex']\n",
        "    if alternateLex is not None and matchesArg(alternateLex, result):\n",
        "        return True\n",
        "\n",
        "    if 'staticMemberAccessLex' in test:\n",
        "        if matchesArg(test['staticMemberAccessLex'], result):\n",
        "            return True\n",
        "            \n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUqA-mJ36SFm"
      },
      "source": [
        "expressionTypes = ['NAME', 'METHOD_INVOC', 'FIELD_ACCESS', 'ARRAY_ACCESS', 'CAST', 'STRING_LIT', 'NUM_LIT', 'CHAR_LIT', 'TYPE_LIT', 'BOOL_LIT',\n",
        "    'NULL_LIT', 'OBJ_CREATION', 'ARR_CREATION', 'THIS', 'SUPER', 'COMPOUND', 'LAMBDA', 'METHOD_REF']\n",
        "expressionTypeDict = {}\n",
        "\n",
        "for i in range(len(expressionTypes)):\n",
        "    expressionTypeDict[expressionTypes[i]] = i\n",
        "\n",
        "tops = [1, 3, 5, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "os.makedirs('results/' + repo_dir, exist_ok=True)\n",
        "os.makedirs('logs/' + repo_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "L82NnK3naslK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b96CurBBe5ET"
      },
      "source": [
        "dataFrame = defaultdict(list)\n",
        "\n",
        "def updateTopKResult(test, results, k, adequateGeneratedCandidate, doPrintIncorrectPrediction, projectName):\n",
        "    isOverallCorrectTopK = False\n",
        "    for i in range(min(k, len(results))):\n",
        "        if canAcceptResult(test, results[i]):\n",
        "            isOverallCorrectTopK = True\n",
        "            break\n",
        "\n",
        "    argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else 'null'\n",
        "    if isOverallCorrectTopK:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(1)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(1)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(1)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(1)\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(1)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(1)\n",
        "    else:\n",
        "        dataFrame[f'GPTActualTop{k}'].append(0)\n",
        "        dataFrame[f'GPTActualTop{k}{argType}'].append(0)\n",
        "\n",
        "        if not test['ignored']:\n",
        "            dataFrame[f'GPTOverallTop{k}'].append(0)\n",
        "            dataFrame[f'GPTOverallTop{k}{argType}'].append(0)\n",
        "            if doPrintIncorrectPrediction:\n",
        "                outputFileName = f\"{projectName}_incorrect_ArgRecTests_top_{k}.txt\"\n",
        "                with open(f'logs/{repo_dir}{outputFileName}', \"a\") as f:\n",
        "                    f.write(json.dumps(test['argRecTestList'][0]) + '\\n')\n",
        "                    f.write('Predictions: ' + str(results) + '\\n')\n",
        "\n",
        "        if adequateGeneratedCandidate:\n",
        "            dataFrame[f'GPTTop{k}'].append(0)\n",
        "            dataFrame[f'GPTTop{k}{argType}'].append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "W_GPT = 1\n",
        "W_LOCALITY = 1\n",
        "\n",
        "def combine_score(main_score, filter_score, candidate):\n",
        "    return main_score * W_GPT + filter_score * (1 - W_GPT)\n",
        "    #return np.log(np.exp(main_score) * W_GPT + np.exp(filter_score) * (1 - W_GPT))\n",
        "    #return max(main_score, filter_score)\n",
        "    #return min(main_score, filter_score)\n",
        "    #return score_by_type(main_score, filter_score, candidate)\n",
        "\n",
        "def combine_score_all_feature(lex_score, locality_score):\n",
        "    return (lex_score + W_LOCALITY * locality_score) / (1 + W_LOCALITY)"
      ],
      "metadata": {
        "id": "OjGHglP0EFv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dcdb86d189814622a0ea9e3502def112",
            "390f5ad0bd3c4a658290dc45915a2b5a",
            "afc8720cf4974762aecff9b0650a6e7a",
            "53e256cdf8784bcab66e6bcd6fefc406",
            "79eabc1b42054c9fbe4c6e18b6039a2b",
            "f11b1ba806954e5185c0c64f5f5f501b",
            "1c1d4759b8184917bca617243de07bc9",
            "ddad900044d741d0bbf5c21b657a23fe",
            "684238a056b64b7890207928bd0c08d0",
            "803025e426544e409dc02cff30e02949",
            "19e3950c982849feb14e87ab8f27e13d"
          ]
        },
        "id": "DCzjVqmp-3Eg",
        "outputId": "69602303-6c6b-4171-a183-ca8f3b37549a"
      },
      "source": [
        "from tqdm.notebook import tqdm as tqdm\n",
        "\n",
        "COMPOUND_CONSIDERED = False\n",
        "TEST_INTRA_INTER_MCALL = None\n",
        "if TEST_INTRA_INTER_MCALL is not None:\n",
        "    if TEST_INTRA_INTER_MCALL:\n",
        "        model += \"__intra_mcall\"\n",
        "    else:\n",
        "        model += \"__inter_mcall\"\n",
        "TEST_LOCAL_ARG = None\n",
        "if TEST_LOCAL_ARG is not None:\n",
        "    if TEST_LOCAL_ARG:\n",
        "        model += \"__local_var\"\n",
        "    else:\n",
        "        model += \"__instance_var_class_var\"\n",
        "\n",
        "rank_list = []\n",
        "\n",
        "# for predId in tqdm(range(len(os.listdir(pred_path)))):\n",
        "#     predFile = os.listdir(pred_path)[predId]\n",
        "#     projectName = predFile[:predFile.find(\"_ArgRecs\")]\n",
        "for projectId in tqdm(range(len(os.listdir(tests_path)))):\n",
        "    testFile = os.listdir(tests_path)[projectId]\n",
        "    projectName = testFile[:testFile.find(\"_ArgRecTests\")]\n",
        "    #print(projectName)\n",
        "    tests = readTests(projectName)\n",
        "\n",
        "    tests = tests[: len(tests) * PREDICT_PERCENT // 100]\n",
        "\n",
        "    tests = allTestsToSingleArgRecTest(tests)\n",
        "    predictions = readPredictions(projectName)\n",
        "    if predictions is None:\n",
        "        print(f\"Predictions for {projectName} not found!\")\n",
        "        continue\n",
        "    filters_predictions = readFilterPreds(projectName)\n",
        "    filters_predictions = correctPredsOrder(filters_predictions, readMapping(projectName))\n",
        "    preprocess_all_filter_preds(filters_predictions)\n",
        "\n",
        "    assert len(tests) == len(predictions), \"Tests not matched!\"\n",
        "    for i in range(len(tests)):\n",
        "        assert tests[i]['expected_lex'][:3] == predictions[i]['answer'][:3], \"Tests not matched!\"\n",
        "\n",
        "    #assert len(tests) == len(filters_predictions), \"Tests not matched!\"\n",
        "    for i in range(len(tests)):\n",
        "        if tests[i]['expected_lex'][:3] != filters_predictions[i]['answer'][:3]:\n",
        "            print(tests[i]['expected_lex'])\n",
        "            print(filters_predictions[i]['answer'])\n",
        "        assert tests[i]['expected_lex'][:3] == filters_predictions[i]['answer'][:3], \"Tests not matched!\"\n",
        "\n",
        "    if TEST_INTRA_INTER_MCALL is not None:\n",
        "        appendices = readAppendices(projectName)\n",
        "        if appendices is None:\n",
        "            print(f\"Appendices for {projectName} not found!\")\n",
        "            continue\n",
        "        mcall_type_set = set()\n",
        "        for appendice in appendices:\n",
        "            if TEST_INTRA_INTER_MCALL:\n",
        "                if appendice['methodInvocOrigin'] == \"src\":\n",
        "                    mcall_type_set.add(f\"{appendice['filePath']}-{appendice['line']}-{appendice['col']}\")\n",
        "            else:\n",
        "                if appendice['methodInvocOrigin'] != \"src\":\n",
        "                    mcall_type_set.add(f\"{appendice['filePath']}-{appendice['line']}-{appendice['col']}\")\n",
        "\n",
        "    for i in range(len(tests)):\n",
        "        test = tests[i]\n",
        "        dataFrame['Tested'].append(1)\n",
        "\n",
        "        if test['numArg'] == 0:\n",
        "            continue\n",
        "        if TEST_INTRA_INTER_MCALL is not None:\n",
        "            if f\"{test['filePath']}-{test['line']}-{test['col']}\" not in mcall_type_set:\n",
        "              continue\n",
        "        if TEST_LOCAL_ARG is not None:\n",
        "            if not predictions[i]['sufficient_candidates']:\n",
        "                continue\n",
        "            if test['argRecTestList'][0]['argType'] != 'NAME':\n",
        "                continue\n",
        "            is_local_arg = False\n",
        "            for j in range(len(test['next_lex'][0])):\n",
        "                for k in range(len(test['next_lex'][0][j])):\n",
        "                    candidate = test['next_lex'][0][j][k]\n",
        "                    if candidate == test['expected_lex']:\n",
        "                        candidate_locality = test['argRecTestList'][0]['candidates_locality'][j][k]\n",
        "                        if candidate_locality >= 4:\n",
        "                            is_local_arg = True\n",
        "                            break\n",
        "                if is_local_arg:\n",
        "                    break\n",
        "            if TEST_LOCAL_ARG != is_local_arg:\n",
        "                continue\n",
        "\n",
        "        dataFrame['Predicted'].append(1)\n",
        "        if not test['ignored']:\n",
        "            dataFrame['Predicted supported'].append(1)\n",
        "\n",
        "        oneArgTest = test['argRecTestList'][0]\n",
        "        next_lex_locality_dict = {}\n",
        "        for j in range(len(oneArgTest['next_lex'])):\n",
        "            for k in range(len(oneArgTest['next_lex'][j])):\n",
        "                candidate = oneArgTest['next_lex'][j][k]\n",
        "                scope_distance = oneArgTest['candidates_scope_distance'][j][k]\n",
        "                lu_distance = oneArgTest['candidates_last_usage_distance'][j][k]\n",
        "                if scope_distance >= 0:\n",
        "                    next_lex_locality_dict[candidate] = def_recent_dict[scope_distance]\n",
        "                    #next_lex_locality_dict[candidate] *= use_recent_dict[lu_distance]\n",
        "        \n",
        "        response = predictions[i]\n",
        "        gptResults = response['predictions']\n",
        "        gptScores = response['scores']\n",
        "        if not COMPOUND_CONSIDERED:\n",
        "            for k in range(len(response['predictions'])):\n",
        "                if response['predictions'][k] == '<COMPOUND>':\n",
        "                    gptResults = gptResults[:k] + gptResults[k + 1:]\n",
        "                    gptScores = gptScores[:k] + gptScores[k + 1:]\n",
        "\n",
        "        prediction_dict = {}\n",
        "        lex_sim_candidate_dict = {}\n",
        "        for j in range(len(filters_predictions[i]['predictions']) - 1, -1, -1):\n",
        "            candidate = filters_predictions[i]['predictions'][j]\n",
        "            prediction_dict[candidate] = filters_predictions[i]['lexModelScores'][j]\n",
        "            if filters_predictions[i]['lexModelScores'][j] > 0:\n",
        "                prediction_dict[candidate] = LOG_ZERO\n",
        "\n",
        "            lex_sim = filters_predictions[i]['lexSimScores'][j]\n",
        "            # lex_sim_score = np.log(lex_sim_dict[get_bin(np.exp(lex_sim))])\n",
        "            # lex_sim_candidate_dict[candidate] = lex_sim_score\n",
        "\n",
        "        for j in range(len(gptResults)):\n",
        "            prediction_dict[gptResults[j]] = combine_score(gptScores[j], prediction_dict[gptResults[j]], gptResults[j])\n",
        "            # prediction_dict[gptResults[j]] = prediction_dict[gptResults[j]] + lex_sim_candidate_dict[gptResults[j]] * 0.5\n",
        "            # prediction_dict[gptResults[j]] = prediction_dict[gptResults[j]] + lex_sim * 0.5\n",
        "\n",
        "            if gptResults[j] in next_lex_locality_dict:\n",
        "                prediction_dict[gptResults[j]] = combine_score_all_feature(prediction_dict[gptResults[j]], np.log(next_lex_locality_dict[gptResults[j]]))\n",
        "\n",
        "        combinedResults = sorted(list(set(filters_predictions[i]['predictions'])), key=lambda x: -prediction_dict[x])\n",
        "\n",
        "        rank = -1\n",
        "        for k in range(min(10, len(combinedResults))):\n",
        "            if canAcceptResult(test, combinedResults[k]):\n",
        "                rank = k\n",
        "        rank_list.append(rank)\n",
        "\n",
        "        for k in tops:\n",
        "            updateTopKResult(test, combinedResults, k, response['sufficient_candidates'], False, projectName)\n",
        "\n",
        "        dataFrame[\"GPT's runtime\"].append(response['runtime'])\n",
        "        argType = test['argRecTestList'][0]['argType'] if 'argType' in test['argRecTestList'][0] else None\n",
        "        if argType is not None:\n",
        "            dataFrame[\"ArgType\"].append(expressionTypeDict[argType])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcdb86d189814622a0ea9e3502def112",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:135: RuntimeWarning: divide by zero encountered in log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MRR = 0\n",
        "for rank in rank_list:\n",
        "    if rank < 0:\n",
        "        MRR += 0\n",
        "    else:\n",
        "        MRR += 1/(rank + 1)\n",
        "MRR /= len(rank_list)\n",
        "print(MRR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf1Uzu_hBotA",
        "outputId": "7dec8200-79e1-4f89-a979-ad43401d22c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7095327595396028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtmIUfQaNrr-"
      },
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('logs/dataframe.pkl', 'wb') as f:\n",
        "#     pickle.dump(dataFrame, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXCB5cWdvYKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557e147d-728c-45c9-f751-4c52e9a86c6c"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def printTestResult():\n",
        "    with open(f'results/{repo_dir}arg_rec_{model}_log.txt', 'w') as f:\n",
        "        f.write(f\"Ran {len(dataFrame['Tested'])} tests successfully.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted'])} tests.\\n\")\n",
        "        f.write(f\"Predicted {len(dataFrame['Predicted supported'])} tests that were supported.\\n\")\n",
        "        f.write(f\"Skipped {len(dataFrame['Tested']) - len(dataFrame['Predicted'])} tests. They were not taken into account during evaluation.\\n\")\n",
        "        gptRuntime = np.mean(dataFrame[\"GPT's runtime\"])\n",
        "        f.write(f\"GPT's runtime: {gptRuntime}s\\n\")\n",
        "        f.write(f\"MRR: {MRR}\\n\")\n",
        "\n",
        "    accuracyPerNumArg = []\n",
        "    row = []\n",
        "    row.append(\"Number of params\")\n",
        "    row.append(\"Percentage of distribution\")\n",
        "    for k in tops:\n",
        "        row.append(f\"GPT's top-{k} accuracy\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} precision\")\n",
        "    for k in tops:\n",
        "        row.append(f\"Top-{k} recall\")\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    unique, counts = np.unique(dataFrame['ArgType'], return_counts=True)\n",
        "    counts = counts / counts.sum()\n",
        "    argTypeDict = defaultdict(float)\n",
        "    for i in range(len(unique)):\n",
        "        argTypeDict[unique[i]] = counts[i]\n",
        "\n",
        "    for i in range(len(expressionTypes)):\n",
        "        argType = expressionTypes[i]\n",
        "        row = []\n",
        "        row.append(argType)\n",
        "        row.append(argTypeDict[i] * 100)\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTOverallTop{k}{argType}\"]))\n",
        "        for k in tops:\n",
        "            row.append(np.mean(dataFrame[f\"GPTActualTop{k}{argType}\"]))\n",
        "        accuracyPerNumArg.append(row)\n",
        "\n",
        "    row = []\n",
        "    row.append(\"all\")\n",
        "    row.append(\"100\")\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTOverallTop{k}\"]))\n",
        "    for k in tops:\n",
        "        row.append(np.mean(dataFrame[f\"GPTActualTop{k}\"]))\n",
        "    accuracyPerNumArg.append(row)\n",
        "\n",
        "    with open(f'results/{repo_dir}arg_rec_{model}.csv', 'w') as f:\n",
        "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        for row in accuracyPerNumArg:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    with open(f'results/{repo_dir}arg_rec_{model}_log.txt', 'a') as f:\n",
        "        for k in tops:\n",
        "            correctTestsCount = np.sum(dataFrame[f\"GPTActualTop{k}\"])\n",
        "            f.write(f\"Target showed up in top {k} recommendations in {correctTestsCount} tests.\\n\")\n",
        "\n",
        "printTestResult()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4neBPx5Q4ecd"
      },
      "source": [
        "for text_file in os.listdir('results/' + repo_dir):\n",
        "    shutil.copyfile('results/' + repo_dir + text_file, main_path + 'results/' + repo_dir + text_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gb-Qex8wPUXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}